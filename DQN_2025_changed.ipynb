{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKbARv7mLj6d"
   },
   "source": [
    "This is an example of running a deep reinforcement learning algorithm. This file is created for a course at the University of Tokyo.\n",
    "\n",
    "In the following, we run the simplest version of Deep Q-Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dH3aro2PDbB"
   },
   "source": [
    "# Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gsSgkbTYLjDZ"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, action_dim)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = state.to(device)\n",
    "        q1 = F.relu(self.l1(state))\n",
    "        q1 = F.relu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "        return q1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fp9I_Of9PUss"
   },
   "source": [
    "# Agent Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxwu3kbqLzwU"
   },
   "source": [
    "We then define the class for DQN. The update of the actor and critic is defined in \"def train()\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "geXgsZXqL5f2"
   },
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            state_dim,\n",
    "            action_dim,\n",
    "            discount=0.99,\n",
    "            tau=0.005,\n",
    "            target_update = 10\n",
    "    ):\n",
    "\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target = copy.deepcopy(self.critic).to(device)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=1e-4)\n",
    "\n",
    "        self.action_dim = action_dim\n",
    "        self.discount = discount\n",
    "        self.tau = tau\n",
    "\n",
    "        self.target_update = target_update\n",
    "\n",
    "        self.total_it = 0\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        with torch.no_grad():\n",
    "          q_values = self.critic(state)\n",
    "          q_values = q_values.squeeze(1)\n",
    "          action = torch.argmax(q_values, dim=1)\n",
    "        return action.cpu().numpy()[0]\n",
    "\n",
    "    def train(self, replay_buffer, batch_size=128):\n",
    "        self.total_it += 1\n",
    "\n",
    "        # Sample replay buffer\n",
    "        state, action_ind, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
    "\n",
    "        # Move all tensors to CUDA\n",
    "        state = state.to(device)\n",
    "        action_ind = action_ind.to(device)\n",
    "        next_state = next_state.to(device)\n",
    "        reward = reward.to(device)\n",
    "        not_done = not_done.to(device)\n",
    "\n",
    "        # one_hot_action = torch.nn.functional.one_hot(action_ind, num_classes=self.action_dim).float()\n",
    "        one_hot_action = torch.nn.functional.one_hot(action_ind, num_classes=int(self.action_dim)).float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_Q = self.critic_target(next_state)\n",
    "            target_Q_max, _ = torch.max(target_Q, dim=1, keepdim=True)\n",
    "            target_Q = reward + not_done * self.discount * target_Q_max\n",
    "\n",
    "        current_Q = self.critic(state)\n",
    "\n",
    "        # print('current_Q', current_Q.shape)\n",
    "        # print('action_ind', action_ind.shape)\n",
    "        # print('target_Q', target_Q.shape)\n",
    "        current_Q_chosen = current_Q.gather(1, action_ind)\n",
    "        # print('current_Q_chosen', current_Q_chosen.shape)\n",
    "\n",
    "        critic_loss = torch.nn.functional.mse_loss(current_Q_chosen, target_Q)\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "    def save_model(self, iter, seed, env_name, foldername='./model/dqn'):\n",
    "        try:\n",
    "            import pathlib\n",
    "            pathlib.Path(foldername).mkdir(parents=True, exist_ok=True)\n",
    "            torch.save(self.actor.state_dict(),\n",
    "                       foldername + '/DQN_'+ env_name +\n",
    "                       '_seed' + str(seed) + '_iter' + str(iter) + '.pth')\n",
    "\n",
    "            print('models is saved for iteration', iter)\n",
    "\n",
    "        except:\n",
    "            print(\"A result directory does not exist and cannot be created. The trial results are not saved\")\n",
    "\n",
    "    def load_model(self, iter, seed, env_name, foldername='model/dqn'):\n",
    "        self.critic.load_state_dict(torch.load(foldername + '/DQN_'+ env_name +\n",
    "                       '_seed' + str(seed) + '_iter' + str(iter) + '.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fl4Rcj-QPL96"
   },
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQ6RlcwhMD7W"
   },
   "source": [
    "The samples collected through trials and errors are stored in the replay buffer. \"def sample()\" is a function that randomly samples a batch of samples from the replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gL1UabbFMIj4"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "\n",
    "        self.state = np.zeros((max_size, state_dim))\n",
    "        self.action = np.zeros((max_size, action_dim))\n",
    "        self.next_state = np.zeros((max_size, state_dim))\n",
    "        self.reward = np.zeros((max_size, 1))\n",
    "        self.not_done = np.zeros((max_size, 1))\n",
    "\n",
    "    def add(self, state, action, next_state, reward, done):\n",
    "        self.state[self.ptr] = state\n",
    "        self.action[self.ptr] = action\n",
    "        self.next_state[self.ptr] = next_state\n",
    "        self.reward[self.ptr] = reward\n",
    "        self.not_done[self.ptr] = 1. - done\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, self.size, size=batch_size)\n",
    "\n",
    "        return (torch.tensor(self.state[ind], dtype=torch.float32),\n",
    "          torch.tensor(self.action[ind], dtype=torch.int64), # Change dtype to torch.int64\n",
    "          torch.tensor(self.next_state[ind], dtype=torch.float32),\n",
    "          torch.tensor(self.reward[ind], dtype=torch.float32),\n",
    "          torch.tensor(self.not_done[ind], dtype=torch.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlBAprkSPdTB"
   },
   "source": [
    "# Training and Evaluating Procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eoq1FNk2MTUK"
   },
   "source": [
    "We define a function for evaluating the policy. When evaluating the trained policy, we evaluate the performance without the exploration noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ttgZ-RrzMWem"
   },
   "outputs": [],
   "source": [
    "def evaluate_greedy(env_test, agent, args, test_iter, test_n, state_dim, trial_seed):\n",
    "\n",
    "    # env_test.reset(seed=1234 + test_n)\n",
    "    # env_seed = (trial_seed - 1234) * 10000 + 1234 + test_n + 10000\n",
    "    # state_test, info = env_test.reset()\n",
    "    state_test, info = env_test.reset(seed=trial_seed + test_iter + test_n)\n",
    "\n",
    "    return_epi_test = 0\n",
    "    for t_test in range(int(args['max_episode_len'])):\n",
    "        action_test = agent.select_action(np.reshape(state_test, (1, state_dim)))\n",
    "        # print(\"====\", action_test)\n",
    "        action_test = int(action_test)\n",
    "        # print(\"!!!\", action_test)\n",
    "        state_test2, reward_test, terminal_test, truncated, info = env_test.step(action_test)\n",
    "        state_test = state_test2\n",
    "        return_epi_test = return_epi_test + reward_test\n",
    "        if terminal_test:\n",
    "            break\n",
    "\n",
    "    print('test_iter:{:d}, nn:{:d}, return_epi_test: {:d}'.format(int(test_iter), int(test_n),\n",
    "                                                                      int(return_epi_test)))\n",
    "\n",
    "    return return_epi_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "5QG6jmjIMbPU"
   },
   "outputs": [],
   "source": [
    "def train(env, env_test, agent, args, index, trial_seed):\n",
    "\n",
    "    # Initialize replay memory\n",
    "    total_step_cnt = 0\n",
    "    epi_cnt = 0\n",
    "    test_iter = 0\n",
    "    return_test = np.zeros((np.ceil(int(args['total_step_num']) / int(args['eval_step_freq'])).astype('int') + 1))\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim =  env.action_space.n\n",
    "\n",
    "    replay_buffer = ReplayBuffer(state_dim, 1)\n",
    "\n",
    "    while total_step_cnt in range( int(args['total_step_num']) ):\n",
    "        # Reset environment\n",
    "        # state, info = env.reset()\n",
    "        state, info = env.reset(seed=trial_seed + epi_cnt)\n",
    "\n",
    "        ep_reward = 0\n",
    "        T_end = False\n",
    "\n",
    "        for t in range(int(args['max_episode_len'])):\n",
    "\n",
    "            # Select action randomly or according to policy\n",
    "            rnd_sample = np.random.uniform(0,1)\n",
    "            if total_step_cnt < int(args['start_timesteps']) or rnd_sample < args['expl_eps']:\n",
    "                action = env.action_space.sample()\n",
    "\n",
    "            else:\n",
    "                action = agent.select_action(np.reshape(state, (1, state_dim)))\n",
    "\n",
    "            # print(\"---\", action)\n",
    "            action = int(action)\n",
    "            # print(\"???\", action)\n",
    "            state2, reward, done, truncated, info_ = env.step(action)\n",
    "\n",
    "            done = float(done) if t < int(args['max_episode_len']) else 1\n",
    "\n",
    "            # Store data in replay buffer\n",
    "            replay_buffer.add(state, action, state2, reward, done)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = state2\n",
    "\n",
    "            # Train agent after collecting sufficient data\n",
    "            if total_step_cnt >= int(args['start_timesteps']):\n",
    "                for i in range(int(args['update_freq'])):\n",
    "                    agent.train(replay_buffer, int(args['batch_size']))\n",
    "\n",
    "            if t == int(args['max_episode_len']) - 1:\n",
    "                T_end = True\n",
    "\n",
    "            ep_reward += reward\n",
    "            total_step_cnt += 1\n",
    "\n",
    "            # Evaluate the deterministic policy\n",
    "            if total_step_cnt >= test_iter * int(args['eval_step_freq']) or total_step_cnt == 1:\n",
    "                print('total_step_cnt', total_step_cnt)\n",
    "                print('evaluating the deterministic policy...')\n",
    "                for test_n in range(int(args['test_num'])):\n",
    "                    return_epi_test = evaluate_greedy(env_test, agent, args, test_iter, test_n, state_dim, trial_seed)\n",
    "\n",
    "                    # Store the average of returns over the test episodes\n",
    "                    return_test[test_iter] = return_test[test_iter] + return_epi_test / float(args['test_num'])\n",
    "\n",
    "                print('return_test[{:d}] {:d}'.format(int(test_iter), int(return_test[test_iter])))\n",
    "                test_iter += 1\n",
    "\n",
    "            if total_step_cnt % int(args['model_save_freq'])==0:\n",
    "                    agent.save_model(iter=test_iter, seed=int(index), env_name=args['env'])\n",
    "\n",
    "\n",
    "            if done or T_end:\n",
    "                epi_cnt += 1\n",
    "                print('| Reward: {:d} | Episode: {:d} | Total step num: {:d} |'.format(int(ep_reward), epi_cnt, total_step_cnt ))\n",
    "                break\n",
    "\n",
    "    return return_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgM2J5a5MjES"
   },
   "source": [
    "Main funciton. To manage the hyperparamters, we use argeparse.\n",
    "We use a task in OpenAI Gym (https://gym.openai.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "sTu9-c77Mk3y",
    "outputId": "564d39a3-eb37-4bc9-813f-cfd9ecba4ff9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "def set_all_seeds(seed=1234):\n",
    "    \"\"\"set all random seed\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    import os\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# 在程序开始时设置种子\n",
    "set_all_seeds(1234)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA not available, using CPU\")\n",
    "\n",
    "parser = argparse.ArgumentParser(description='provide arguments')\n",
    "\n",
    "# run parameters\n",
    "parser.add_argument('--env', help='choose the gym env- tested on {CartPole-v1}')\n",
    "parser.add_argument('--env-id', type=int, default=0, help='choose the gym env- tested on {CartPole-v1}')\n",
    "parser.add_argument('--random-seed', help='random seed for repeatability', default=1234)\n",
    "parser.add_argument('--max-episode-len', help='max length of 1 episode', default=1000)\n",
    "parser.add_argument('--trial-num', help='number of trials', default=3)\n",
    "parser.add_argument('--total-step-num', help='total number of time steps', default=20000)\n",
    "parser.add_argument('--eval-step-freq', help='frequency of evaluating the policy', default=2500)\n",
    "parser.add_argument('--test-num', help='number of test episodes', default=10)\n",
    "\n",
    "parser.add_argument('--result-file', help='file name for storing results from multiple trials',\n",
    "                    default='/trials_dqn_')\n",
    "parser.add_argument('--trial-idx', help='index of trials', default=0)\n",
    "parser.add_argument('--model-save-freq', help='frequency of evaluating the policy', default=20000)\n",
    "parser.add_argument('--model-folder',  default='./model/dqn')\n",
    "\n",
    "parser.add_argument(\"--start_timesteps\", default=1e4, type=int)  # How many time steps purely random policy is run for\n",
    "parser.add_argument(\"--update_freq\", default=1, type=int)  # Number of policy updates\n",
    "parser.add_argument(\"--expl-eps\", default=0.1, type=float)  # epsilon for epsilon-greedy\n",
    "parser.add_argument(\"--batch-size\", default=128, type=int)  # Batch size for both actor and critic\n",
    "parser.add_argument(\"--discount\", default=0.99, type=float)  # Discount factor\n",
    "parser.add_argument(\"--tau\", default=0.005, type=float)  # Target network update rate\n",
    "\n",
    "args_tmp, unknown = parser.parse_known_args()\n",
    "\n",
    "if args_tmp.env is None:\n",
    "    env_dict = {0 : \"CartPole-v1\",\n",
    "    }\n",
    "    args_tmp.env = env_dict[args_tmp.env_id]\n",
    "args = vars(args_tmp)\n",
    "\n",
    "return_set = []\n",
    "# for ite in range(int(args['trial_num'])):\n",
    "#     print('Trial Number:', ite)\n",
    "\n",
    "#     index = int(ite) + int(args['trial_idx'])\n",
    "#     env = gym.make(args['env'])\n",
    "#     env.reset(seed=1234)\n",
    "\n",
    "#     np.random.seed(index )\n",
    "#     env_test = gym.make(args['env'])\n",
    "#     env_test.reset(seed=1234)\n",
    "\n",
    "#     state_dim = env.observation_space.shape[0]\n",
    "#     n_actions = env.action_space.n\n",
    "\n",
    "#     agent = DQN(state_dim=state_dim, action_dim=n_actions, discount=args['discount'], tau=args['tau'])\n",
    "\n",
    "#     step_R_i = train(env, env_test, agent, args, index)\n",
    "#     return_set.append(step_R_i)\n",
    "\n",
    "#     result_path = \"./results/trials/dqn\"\n",
    "#     result_filename = result_path + args['result_file'] +  args['env'] +  '_trial_idx_' + str(index) + '.txt'\n",
    "#     try:\n",
    "#         import pathlib\n",
    "#         pathlib.Path(result_path).mkdir(parents=True, exist_ok=True)\n",
    "#         np.savetxt(result_filename, np.asarray(step_R_i))\n",
    "#         print('The result of the trial no.' + str(index) +' was saved.')\n",
    "#     except:\n",
    "#         print(\"A result directory does not exist and cannot be created. The trial results are not saved\")\n",
    "\n",
    "for ite in range(int(args['trial_num'])):\n",
    "    print('Trial Number:', ite)\n",
    "    \n",
    "    # set different seed for each trial\n",
    "    trial_seed = int(args['random_seed']) + ite\n",
    "    set_all_seeds(trial_seed)\n",
    "    \n",
    "    index = int(ite) + int(args['trial_idx'])\n",
    "    \n",
    "    env = gym.make(args['env'])\n",
    "    env.action_space.seed(trial_seed)\n",
    "    env.observation_space.seed(trial_seed)\n",
    "    \n",
    "    env_test = gym.make(args['env'])\n",
    "    env_test.action_space.seed(trial_seed)\n",
    "    env_test.observation_space.seed(trial_seed)\n",
    "    \n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    agent = DQN(state_dim=state_dim, action_dim=n_actions, discount=args['discount'], tau=args['tau'])\n",
    "    \n",
    "    step_R_i = train(env, env_test, agent, args, index, trial_seed)\n",
    "    return_set.append(step_R_i)\n",
    "    \n",
    "    result_path = \"./results/trials/dqn\"\n",
    "    result_filename = result_path + args['result_file'] + args['env'] + '_trial_idx_' + str(index) + '.txt'\n",
    "    try:\n",
    "        import pathlib\n",
    "        pathlib.Path(result_path).mkdir(parents=True, exist_ok=True)\n",
    "        np.savetxt(result_filename, np.asarray(step_R_i))\n",
    "        print('The result of the trial no.' + str(index) + ' was saved.')\n",
    "    except:\n",
    "        print(\"A result directory does not exist and cannot be created. The trial results are not saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kmuxBsUUk27"
   },
   "source": [
    "We plot the results of the training using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "q4FMseCqUkfr",
    "outputId": "d3fcc3f4-3968-450b-8c99-bff18a44ff5d"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib\n",
    "\n",
    "figure(figsize=(7, 6))\n",
    "\n",
    "t = np.arange(0, int(args['total_step_num']) + 1, int(args['eval_step_freq'])) * 0.001\n",
    "\n",
    "mean = np.mean(np.asarray(return_set), axis=0)\n",
    "std = np.std(np.asarray(return_set), axis=0)\n",
    "color = 'b'\n",
    "label = 'DQN'\n",
    "plt.plot(t, mean, color, label=label)\n",
    "plt.fill(np.concatenate([t, t[::-1]]), np.concatenate([mean - 1.9600 * std,\n",
    "                                      (mean + 1.9600 * std)[::-1]]), alpha=.1, fc=color, ec='None')\n",
    "\n",
    "plt.xlabel('Time steps [x 1e3]', fontsize=14)\n",
    "plt.ylabel('Return', fontsize=14)\n",
    "plt.legend(loc='lower right', fontsize=14)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "inv_opt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
