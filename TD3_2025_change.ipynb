{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLRdee6I7tw3"
   },
   "source": [
    "This is an example of running a deep reinforcement learning algorithm.\n",
    "In the following, we run Twin Delayed Deep Deterministic Policy Gradient (TD3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0lwnQCWhElA"
   },
   "source": [
    "# Neural Network Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDbjVC6oalVN"
   },
   "source": [
    "We define classes for the actor and critic networks. In TD3, we prepare two neural networks for the critic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_all_seeds(seed=1234):\n",
    "    \"\"\"set all random seed\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    import os\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "set_all_seeds(1234)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA not available, using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iQj4wEKr7jWv"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Actor(nn.Module):\n",
    "\tdef __init__(self, state_dim, action_dim, max_action):\n",
    "\t\tsuper(Actor, self).__init__()\n",
    "\n",
    "\t\tself.l1 = nn.Linear(state_dim, 256)\n",
    "\t\tself.l2 = nn.Linear(256, 256)\n",
    "\t\tself.l3 = nn.Linear(256, action_dim)\n",
    "\n",
    "\t\tself.max_action = max_action\n",
    "\n",
    "\n",
    "\tdef forward(self, state):\n",
    "\t\ta = F.relu(self.l1(state))\n",
    "\t\ta = F.relu(self.l2(a))\n",
    "\t\treturn self.max_action * torch.tanh(self.l3(a))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "\tdef __init__(self, state_dim, action_dim):\n",
    "\t\tsuper(Critic, self).__init__()\n",
    "\n",
    "\t\t# Q1 architecture\n",
    "\t\tself.l1 = nn.Linear(state_dim + action_dim, 256)\n",
    "\t\tself.l2 = nn.Linear(256, 256)\n",
    "\t\tself.l3 = nn.Linear(256, 1)\n",
    "\n",
    "\t\t# Q2 architecture\n",
    "\t\tself.l4 = nn.Linear(state_dim + action_dim, 256)\n",
    "\t\tself.l5 = nn.Linear(256, 256)\n",
    "\t\tself.l6 = nn.Linear(256, 1)\n",
    "\n",
    "\n",
    "\tdef forward(self, state, action):\n",
    "\t\tsa = torch.cat([state, action], 1)\n",
    "\n",
    "\t\tq1 = F.relu(self.l1(sa))\n",
    "\t\tq1 = F.relu(self.l2(q1))\n",
    "\t\tq1 = self.l3(q1)\n",
    "\n",
    "\t\tq2 = F.relu(self.l4(sa))\n",
    "\t\tq2 = F.relu(self.l5(q2))\n",
    "\t\tq2 = self.l6(q2)\n",
    "\t\treturn q1, q2\n",
    "\n",
    "\n",
    "\tdef Q1(self, state, action):\n",
    "\t\tsa = torch.cat([state, action], 1)\n",
    "\n",
    "\t\tq1 = F.relu(self.l1(sa))\n",
    "\t\tq1 = F.relu(self.l2(q1))\n",
    "\t\tq1 = self.l3(q1)\n",
    "\t\treturn q1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pFfw7tE7sBn"
   },
   "source": [
    "We then define the class for TD3. The update of the actor and critic is defined in \"def train()\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kC1xWehEaip1"
   },
   "outputs": [],
   "source": [
    "class TD3(object):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tstate_dim,\n",
    "\t\taction_dim,\n",
    "\t\tmax_action,\n",
    "\t\tdiscount=0.99,\n",
    "\t\ttau=0.005,\n",
    "\t\tpolicy_noise=0.2,\n",
    "\t\tnoise_clip=0.5,\n",
    "\t\tpolicy_freq=2\n",
    "\t):\n",
    "\n",
    "\t\tself.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "\t\tself.actor_target = copy.deepcopy(self.actor)\n",
    "\t\tself.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "\n",
    "\t\tself.critic = Critic(state_dim, action_dim).to(device)\n",
    "\t\tself.critic_target = copy.deepcopy(self.critic)\n",
    "\t\tself.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "\n",
    "\t\tself.max_action = max_action\n",
    "\t\tself.discount = discount\n",
    "\t\tself.tau = tau\n",
    "\t\tself.policy_noise = policy_noise\n",
    "\t\tself.noise_clip = noise_clip\n",
    "\t\tself.policy_freq = policy_freq\n",
    "\n",
    "\t\tself.total_it = 0\n",
    "\n",
    "\n",
    "\tdef select_action(self, state, test=False):\n",
    "\t\tstate = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "\t\treturn self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "\n",
    "\tdef train(self, replay_buffer, batch_size=256):\n",
    "\t\tself.total_it += 1\n",
    "\n",
    "\t\t# Sample replay buffer\n",
    "\t\tstate, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\t# Select action according to policy and add clipped noise\n",
    "\t\t\tnoise = (\n",
    "\t\t\t\ttorch.randn_like(action) * self.policy_noise\n",
    "\t\t\t).clamp(-self.noise_clip, self.noise_clip)\n",
    "\n",
    "\t\t\tnext_action = (\n",
    "\t\t\t\tself.actor_target(next_state) + noise\n",
    "\t\t\t).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "\t\t\t# Compute the target Q value\n",
    "\t\t\ttarget_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "\t\t\ttarget_Q = torch.min(target_Q1, target_Q2)\n",
    "\t\t\ttarget_Q = reward + not_done * self.discount * target_Q\n",
    "\n",
    "\t\t# Get current Q estimates\n",
    "\t\tcurrent_Q1, current_Q2 = self.critic(state, action)\n",
    "\n",
    "\t\t# Compute critic loss\n",
    "\t\tcritic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "\n",
    "\t\t# Optimize the critic\n",
    "\t\tself.critic_optimizer.zero_grad()\n",
    "\t\tcritic_loss.backward()\n",
    "\t\tself.critic_optimizer.step()\n",
    "\n",
    "\t\t# Delayed policy updates\n",
    "\t\tif self.total_it % self.policy_freq == 0:\n",
    "\n",
    "\t\t\t# Compute actor loss\n",
    "\t\t\tactor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "\n",
    "\t\t\t# Optimize the actor\n",
    "\t\t\tself.actor_optimizer.zero_grad()\n",
    "\t\t\tactor_loss.backward()\n",
    "\t\t\tself.actor_optimizer.step()\n",
    "\n",
    "\t\t\t# Update the frozen target models\n",
    "\t\t\tfor param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "\t\t\tfor param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "\n",
    "\tdef save(self, filename):\n",
    "\t\ttorch.save(self.critic.state_dict(), filename + \"_critic\")\n",
    "\t\ttorch.save(self.critic_optimizer.state_dict(), filename + \"_critic_optimizer\")\n",
    "\t\ttorch.save(self.actor.state_dict(), filename + \"_actor\")\n",
    "\t\ttorch.save(self.actor_optimizer.state_dict(), filename + \"_actor_optimizer\")\n",
    "\n",
    "\n",
    "\tdef load(self, filename):\n",
    "\t\tself.critic.load_state_dict(torch.load(filename + \"_critic\"))\n",
    "\t\tself.critic_optimizer.load_state_dict(torch.load(filename + \"_critic_optimizer\"))\n",
    "\t\tself.actor.load_state_dict(torch.load(filename + \"_actor\"))\n",
    "\t\tself.actor_optimizer.load_state_dict(torch.load(filename + \"_actor_optimizer\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWAgIHhdhcYD"
   },
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lm1Og8m4cGDA"
   },
   "source": [
    "The samples collected through trials and errors are stored in the replay buffer. \"def sample()\" is a function that randomly samples a batch of samples from the replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "10_yuayicCbp"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\tdef __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
    "\t\tself.max_size = max_size\n",
    "\t\tself.ptr = 0\n",
    "\t\tself.size = 0\n",
    "\n",
    "\t\tself.state = np.zeros((max_size, state_dim))\n",
    "\t\tself.action = np.zeros((max_size, action_dim))\n",
    "\t\tself.next_state = np.zeros((max_size, state_dim))\n",
    "\t\tself.reward = np.zeros((max_size, 1))\n",
    "\t\tself.not_done = np.zeros((max_size, 1))\n",
    "\n",
    "\t\tself.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\tdef add(self, state, action, next_state, reward, done):\n",
    "\t\tself.state[self.ptr] = state\n",
    "\t\tself.action[self.ptr] = action\n",
    "\t\tself.next_state[self.ptr] = next_state\n",
    "\t\tself.reward[self.ptr] = reward\n",
    "\t\tself.not_done[self.ptr] = 1. - done\n",
    "\n",
    "\t\tself.ptr = (self.ptr + 1) % self.max_size\n",
    "\t\tself.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "\n",
    "\tdef sample(self, batch_size):\n",
    "\t\tind = np.random.randint(self.size, size=batch_size)\n",
    "\n",
    "\t\treturn (\n",
    "\t\t\ttorch.FloatTensor(self.state[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.action[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.next_state[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.reward[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.not_done[ind]).to(self.device)\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6xvdeuChkTE"
   },
   "source": [
    "# Training and evaluating procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhStR6soa5dl"
   },
   "source": [
    "We define a function for evaluating the policy. When evaluating the trained policy, we evaluate the performance without the exploration noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qLp8X2BDbBJh"
   },
   "outputs": [],
   "source": [
    "def eval_policy(policy, env, seed, eval_episodes=10):\n",
    "\teval_env = gym.make(env)\n",
    "\n",
    "\tavg_reward = 0.\n",
    "\tfor _ in range(eval_episodes):\n",
    "\t\tstate, info = eval_env.reset(seed=seed+eval_episodes)          # 固定评测环境\n",
    "\t\teval_env.action_space.seed(seed+eval_episodes)\n",
    "\t\tterminated, truncated = False, False\n",
    "\t\twhile not (terminated or truncated):\n",
    "\t\t\taction = policy.select_action(np.array(state), test=True)\n",
    "\t\t\tstate, reward, terminated, truncated, info  = eval_env.step(action)\n",
    "\t\t\tavg_reward += reward\n",
    "\n",
    "\tavg_reward /= eval_episodes\n",
    "\n",
    "\tprint(\"---------------------------------------\")\n",
    "\tprint(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "\tprint(\"---------------------------------------\")\n",
    "\treturn avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJgOjkxObQbL"
   },
   "source": [
    "Below is the training procedure. We collect samples through trials and errors and store them in the replay buffer. The policy is trained once after every one time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Q-LnStctbHOC"
   },
   "outputs": [],
   "source": [
    "def train(env, agent, args, index, trial_seed):\n",
    "\n",
    "    # Initialize replay memory\n",
    "    total_step_cnt = 0\n",
    "    epi_cnt = 0\n",
    "    test_iter = 0\n",
    "    return_test = np.zeros((np.ceil(int(args['total_step_num']) / int(args['eval_step_freq'])).astype('int') + 1))\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    max_action = float(env.action_space.high[0])\n",
    "\n",
    "    replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
    "    episode_timesteps = 0\n",
    "\n",
    "    while total_step_cnt in range( int(args['total_step_num']) ):\n",
    "\n",
    "        state, info = env.reset(seed=trial_seed + epi_cnt)\n",
    "        ep_reward = 0\n",
    "        T_end = False\n",
    "\n",
    "        for t in range(int(args['max_episode_len'])):\n",
    "\n",
    "            # Select action randomly or according to policy\n",
    "            if total_step_cnt < int(args['start_timesteps']):\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.clip(\n",
    "                        agent.select_action(np.array(state))\n",
    "                        + np.random.normal(0, max_action * float(args['expl_noise']), size=action_dim),\n",
    "                        -max_action, max_action)\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done_bool = float(terminated) if episode_timesteps < env._max_episode_steps else 0\n",
    "\n",
    "            # Store data in replay buffer\n",
    "            replay_buffer.add(state, action, next_state, reward, done_bool)\n",
    "\n",
    "            # Train agent after collecting sufficient data\n",
    "            if total_step_cnt >= int(args['start_timesteps']):\n",
    "                for i in range(int(args['update_freq'])):\n",
    "                    agent.train(replay_buffer, int(args['batch_size']))\n",
    "\n",
    "            if t == int(args['max_episode_len']) - 1:\n",
    "                T_end = True\n",
    "\n",
    "            state = next_state\n",
    "            ep_reward += reward\n",
    "            total_step_cnt += 1\n",
    "\n",
    "            # Evaluate the deterministic policy\n",
    "            if total_step_cnt >= test_iter * int(args['eval_step_freq']) or total_step_cnt == 1:\n",
    "                print('total_step_cnt', total_step_cnt)\n",
    "                print('evaluating the deterministic policy...')\n",
    "                for test_n in range(int(args['test_num'])):\n",
    "                    return_epi_test = eval_policy(agent, args['env'], trial_seed+test_n+test_iter, eval_episodes=args['test_num'])\n",
    "\n",
    "                    # Store the average of returns over the test episodes\n",
    "                    return_test[test_iter] = return_test[test_iter] + return_epi_test / float(args['test_num'])\n",
    "\n",
    "                print('return_test[{:d}] {:d}'.format(int(test_iter), int(return_test[test_iter])))\n",
    "                test_iter += 1\n",
    "\n",
    "            if terminated or truncated:\n",
    "                epi_cnt += 1\n",
    "                print('| Reward: {:d} | Episode: {:d} | Total step num: {:d} |'.format(int(ep_reward), epi_cnt, total_step_cnt ))\n",
    "\n",
    "                break\n",
    "\n",
    "    return return_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxW9O6IUfPeZ"
   },
   "source": [
    "Main funciton. To manage the hyperparamters of TD3, we use argeparse.\n",
    "We use a task in OpenAI Gym (https://gym.openai.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XSJU8BAebYYO",
    "outputId": "c3d51b8a-a757-4e52-81b0-ffb3d6a0ccd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device: NVIDIA GeForce RTX 3090\n",
      "Trial Number: 0\n",
      "action_space.shape (1,)\n",
      "observation_space.shape (3,)\n",
      "total_step_cnt 1\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1382.448\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1280.658\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -976.828\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -978.300\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1336.772\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1860.920\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -988.357\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1054.857\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1725.251\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1007.191\n",
      "---------------------------------------\n",
      "return_test[0] -1259\n",
      "| Reward: -1877 | Episode: 1 | Total step num: 200 |\n",
      "| Reward: -1489 | Episode: 2 | Total step num: 400 |\n",
      "| Reward: -943 | Episode: 3 | Total step num: 600 |\n",
      "| Reward: -860 | Episode: 4 | Total step num: 800 |\n",
      "| Reward: -1845 | Episode: 5 | Total step num: 1000 |\n",
      "| Reward: -1063 | Episode: 6 | Total step num: 1200 |\n",
      "| Reward: -1723 | Episode: 7 | Total step num: 1400 |\n",
      "| Reward: -1184 | Episode: 8 | Total step num: 1600 |\n",
      "| Reward: -1293 | Episode: 9 | Total step num: 1800 |\n",
      "| Reward: -1329 | Episode: 10 | Total step num: 2000 |\n",
      "| Reward: -1302 | Episode: 11 | Total step num: 2200 |\n",
      "| Reward: -1271 | Episode: 12 | Total step num: 2400 |\n",
      "| Reward: -795 | Episode: 13 | Total step num: 2600 |\n",
      "| Reward: -1166 | Episode: 14 | Total step num: 2800 |\n",
      "| Reward: -1457 | Episode: 15 | Total step num: 3000 |\n",
      "| Reward: -1753 | Episode: 16 | Total step num: 3200 |\n",
      "| Reward: -1160 | Episode: 17 | Total step num: 3400 |\n",
      "| Reward: -971 | Episode: 18 | Total step num: 3600 |\n",
      "| Reward: -1601 | Episode: 19 | Total step num: 3800 |\n",
      "| Reward: -1000 | Episode: 20 | Total step num: 4000 |\n",
      "| Reward: -1269 | Episode: 21 | Total step num: 4200 |\n",
      "| Reward: -1585 | Episode: 22 | Total step num: 4400 |\n",
      "| Reward: -1054 | Episode: 23 | Total step num: 4600 |\n",
      "| Reward: -1501 | Episode: 24 | Total step num: 4800 |\n",
      "total_step_cnt 5000\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1280.658\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -976.828\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -978.300\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1336.772\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1860.920\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -988.357\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1054.857\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1725.251\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1007.191\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1011.135\n",
      "---------------------------------------\n",
      "return_test[1] -1222\n",
      "| Reward: -768 | Episode: 25 | Total step num: 5000 |\n",
      "| Reward: -1811 | Episode: 26 | Total step num: 5200 |\n",
      "| Reward: -911 | Episode: 27 | Total step num: 5400 |\n",
      "| Reward: -1110 | Episode: 28 | Total step num: 5600 |\n",
      "| Reward: -1687 | Episode: 29 | Total step num: 5800 |\n",
      "| Reward: -909 | Episode: 30 | Total step num: 6000 |\n",
      "| Reward: -1658 | Episode: 31 | Total step num: 6200 |\n",
      "| Reward: -931 | Episode: 32 | Total step num: 6400 |\n",
      "| Reward: -1139 | Episode: 33 | Total step num: 6600 |\n",
      "| Reward: -1635 | Episode: 34 | Total step num: 6800 |\n",
      "| Reward: -1708 | Episode: 35 | Total step num: 7000 |\n",
      "| Reward: -895 | Episode: 36 | Total step num: 7200 |\n",
      "| Reward: -1623 | Episode: 37 | Total step num: 7400 |\n",
      "| Reward: -1373 | Episode: 38 | Total step num: 7600 |\n",
      "| Reward: -923 | Episode: 39 | Total step num: 7800 |\n",
      "| Reward: -1286 | Episode: 40 | Total step num: 8000 |\n",
      "| Reward: -928 | Episode: 41 | Total step num: 8200 |\n",
      "| Reward: -1455 | Episode: 42 | Total step num: 8400 |\n",
      "| Reward: -1674 | Episode: 43 | Total step num: 8600 |\n",
      "| Reward: -1193 | Episode: 44 | Total step num: 8800 |\n",
      "| Reward: -870 | Episode: 45 | Total step num: 9000 |\n",
      "| Reward: -1156 | Episode: 46 | Total step num: 9200 |\n",
      "| Reward: -1763 | Episode: 47 | Total step num: 9400 |\n",
      "| Reward: -1413 | Episode: 48 | Total step num: 9600 |\n",
      "| Reward: -969 | Episode: 49 | Total step num: 9800 |\n",
      "total_step_cnt 10000\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -976.828\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -978.300\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1336.772\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1860.920\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -988.357\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1054.857\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1725.251\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1007.191\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1011.135\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1719.378\n",
      "---------------------------------------\n",
      "return_test[2] -1265\n",
      "| Reward: -863 | Episode: 50 | Total step num: 10000 |\n",
      "| Reward: -1295 | Episode: 51 | Total step num: 10200 |\n",
      "| Reward: -1533 | Episode: 52 | Total step num: 10400 |\n",
      "| Reward: -1714 | Episode: 53 | Total step num: 10600 |\n",
      "| Reward: -1671 | Episode: 54 | Total step num: 10800 |\n",
      "| Reward: -1694 | Episode: 55 | Total step num: 11000 |\n",
      "| Reward: -1537 | Episode: 56 | Total step num: 11200 |\n",
      "| Reward: -1468 | Episode: 57 | Total step num: 11400 |\n",
      "| Reward: -1475 | Episode: 58 | Total step num: 11600 |\n",
      "| Reward: -1477 | Episode: 59 | Total step num: 11800 |\n",
      "| Reward: -1520 | Episode: 60 | Total step num: 12000 |\n",
      "| Reward: -1516 | Episode: 61 | Total step num: 12200 |\n",
      "| Reward: -1322 | Episode: 62 | Total step num: 12400 |\n",
      "| Reward: -1268 | Episode: 63 | Total step num: 12600 |\n",
      "| Reward: 0 | Episode: 64 | Total step num: 12800 |\n",
      "| Reward: -1057 | Episode: 65 | Total step num: 13000 |\n",
      "| Reward: -986 | Episode: 66 | Total step num: 13200 |\n",
      "| Reward: -1084 | Episode: 67 | Total step num: 13400 |\n",
      "| Reward: -1148 | Episode: 68 | Total step num: 13600 |\n",
      "| Reward: -656 | Episode: 69 | Total step num: 13800 |\n",
      "| Reward: -405 | Episode: 70 | Total step num: 14000 |\n",
      "| Reward: -281 | Episode: 71 | Total step num: 14200 |\n",
      "| Reward: -521 | Episode: 72 | Total step num: 14400 |\n",
      "| Reward: -422 | Episode: 73 | Total step num: 14600 |\n",
      "| Reward: -130 | Episode: 74 | Total step num: 14800 |\n",
      "total_step_cnt 15000\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -125.028\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -267.701\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -387.258\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -125.567\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -128.600\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -605.904\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -125.242\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -126.865\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1491.346\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -125.327\n",
      "---------------------------------------\n",
      "return_test[3] -350\n",
      "| Reward: -290 | Episode: 75 | Total step num: 15000 |\n",
      "| Reward: -276 | Episode: 76 | Total step num: 15200 |\n",
      "| Reward: -268 | Episode: 77 | Total step num: 15400 |\n",
      "| Reward: -123 | Episode: 78 | Total step num: 15600 |\n",
      "| Reward: -117 | Episode: 79 | Total step num: 15800 |\n",
      "| Reward: -123 | Episode: 80 | Total step num: 16000 |\n",
      "| Reward: -128 | Episode: 81 | Total step num: 16200 |\n",
      "| Reward: -123 | Episode: 82 | Total step num: 16400 |\n",
      "| Reward: -1 | Episode: 83 | Total step num: 16600 |\n",
      "| Reward: -2 | Episode: 84 | Total step num: 16800 |\n",
      "| Reward: -258 | Episode: 85 | Total step num: 17000 |\n",
      "| Reward: -253 | Episode: 86 | Total step num: 17200 |\n",
      "| Reward: -123 | Episode: 87 | Total step num: 17400 |\n",
      "| Reward: -233 | Episode: 88 | Total step num: 17600 |\n",
      "| Reward: -1 | Episode: 89 | Total step num: 17800 |\n",
      "| Reward: -120 | Episode: 90 | Total step num: 18000 |\n",
      "| Reward: -115 | Episode: 91 | Total step num: 18200 |\n",
      "| Reward: -350 | Episode: 92 | Total step num: 18400 |\n",
      "| Reward: -125 | Episode: 93 | Total step num: 18600 |\n",
      "| Reward: -243 | Episode: 94 | Total step num: 18800 |\n",
      "| Reward: -127 | Episode: 95 | Total step num: 19000 |\n",
      "| Reward: -334 | Episode: 96 | Total step num: 19200 |\n",
      "| Reward: -1 | Episode: 97 | Total step num: 19400 |\n",
      "| Reward: -126 | Episode: 98 | Total step num: 19600 |\n",
      "| Reward: -355 | Episode: 99 | Total step num: 19800 |\n",
      "total_step_cnt 20000\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -246.304\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -336.733\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -120.648\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -122.340\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -229.147\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -117.200\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -118.866\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -233.796\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -120.346\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -223.269\n",
      "---------------------------------------\n",
      "return_test[4] -186\n",
      "| Reward: -127 | Episode: 100 | Total step num: 20000 |\n",
      "| Reward: -297 | Episode: 101 | Total step num: 20200 |\n",
      "| Reward: -126 | Episode: 102 | Total step num: 20400 |\n",
      "| Reward: -119 | Episode: 103 | Total step num: 20600 |\n",
      "| Reward: -244 | Episode: 104 | Total step num: 20800 |\n",
      "| Reward: -125 | Episode: 105 | Total step num: 21000 |\n",
      "| Reward: -125 | Episode: 106 | Total step num: 21200 |\n",
      "| Reward: -5 | Episode: 107 | Total step num: 21400 |\n",
      "| Reward: -233 | Episode: 108 | Total step num: 21600 |\n",
      "| Reward: -331 | Episode: 109 | Total step num: 21800 |\n",
      "| Reward: -243 | Episode: 110 | Total step num: 22000 |\n",
      "| Reward: -119 | Episode: 111 | Total step num: 22200 |\n",
      "| Reward: -120 | Episode: 112 | Total step num: 22400 |\n",
      "| Reward: -232 | Episode: 113 | Total step num: 22600 |\n",
      "| Reward: -240 | Episode: 114 | Total step num: 22800 |\n",
      "| Reward: -245 | Episode: 115 | Total step num: 23000 |\n",
      "| Reward: -121 | Episode: 116 | Total step num: 23200 |\n",
      "| Reward: -3 | Episode: 117 | Total step num: 23400 |\n",
      "| Reward: -129 | Episode: 118 | Total step num: 23600 |\n",
      "| Reward: -124 | Episode: 119 | Total step num: 23800 |\n",
      "| Reward: -4 | Episode: 120 | Total step num: 24000 |\n",
      "| Reward: -3 | Episode: 121 | Total step num: 24200 |\n",
      "| Reward: -129 | Episode: 122 | Total step num: 24400 |\n",
      "| Reward: -238 | Episode: 123 | Total step num: 24600 |\n",
      "| Reward: -118 | Episode: 124 | Total step num: 24800 |\n",
      "total_step_cnt 25000\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -356.563\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -122.495\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -121.897\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -235.157\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -120.212\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -120.421\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -238.790\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -121.964\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -230.880\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -122.572\n",
      "---------------------------------------\n",
      "return_test[5] -179\n",
      "| Reward: -122 | Episode: 125 | Total step num: 25000 |\n",
      "The result of the trial no.0 was saved.\n",
      "Trial Number: 1\n",
      "action_space.shape (1,)\n",
      "observation_space.shape (3,)\n",
      "total_step_cnt 1\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1233.113\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1458.099\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1491.027\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1256.627\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1810.927\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1403.697\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1541.172\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1547.578\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1342.275\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1522.307\n",
      "---------------------------------------\n",
      "return_test[0] -1460\n",
      "| Reward: -1306 | Episode: 1 | Total step num: 200 |\n",
      "| Reward: -888 | Episode: 2 | Total step num: 400 |\n",
      "| Reward: -769 | Episode: 3 | Total step num: 600 |\n",
      "| Reward: -1534 | Episode: 4 | Total step num: 800 |\n",
      "| Reward: -865 | Episode: 5 | Total step num: 1000 |\n",
      "| Reward: -1599 | Episode: 6 | Total step num: 1200 |\n",
      "| Reward: -1311 | Episode: 7 | Total step num: 1400 |\n",
      "| Reward: -1298 | Episode: 8 | Total step num: 1600 |\n",
      "| Reward: -1557 | Episode: 9 | Total step num: 1800 |\n",
      "| Reward: -1614 | Episode: 10 | Total step num: 2000 |\n",
      "| Reward: -1302 | Episode: 11 | Total step num: 2200 |\n",
      "| Reward: -872 | Episode: 12 | Total step num: 2400 |\n",
      "| Reward: -1054 | Episode: 13 | Total step num: 2600 |\n",
      "| Reward: -1368 | Episode: 14 | Total step num: 2800 |\n",
      "| Reward: -1576 | Episode: 15 | Total step num: 3000 |\n",
      "| Reward: -971 | Episode: 16 | Total step num: 3200 |\n",
      "| Reward: -1204 | Episode: 17 | Total step num: 3400 |\n",
      "| Reward: -1727 | Episode: 18 | Total step num: 3600 |\n",
      "| Reward: -895 | Episode: 19 | Total step num: 3800 |\n",
      "| Reward: -1073 | Episode: 20 | Total step num: 4000 |\n",
      "| Reward: -1812 | Episode: 21 | Total step num: 4200 |\n",
      "| Reward: -1012 | Episode: 22 | Total step num: 4400 |\n",
      "| Reward: -1723 | Episode: 23 | Total step num: 4600 |\n",
      "| Reward: -1047 | Episode: 24 | Total step num: 4800 |\n",
      "total_step_cnt 5000\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1458.099\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1491.027\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1256.627\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1810.927\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1403.697\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1541.172\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1547.578\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1342.275\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1522.307\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1538.181\n",
      "---------------------------------------\n",
      "return_test[1] -1491\n",
      "| Reward: -1630 | Episode: 25 | Total step num: 5000 |\n",
      "| Reward: -790 | Episode: 26 | Total step num: 5200 |\n",
      "| Reward: -1058 | Episode: 27 | Total step num: 5400 |\n",
      "| Reward: -1418 | Episode: 28 | Total step num: 5600 |\n",
      "| Reward: -1027 | Episode: 29 | Total step num: 5800 |\n",
      "| Reward: -1560 | Episode: 30 | Total step num: 6000 |\n",
      "| Reward: -966 | Episode: 31 | Total step num: 6200 |\n",
      "| Reward: -1184 | Episode: 32 | Total step num: 6400 |\n",
      "| Reward: -1664 | Episode: 33 | Total step num: 6600 |\n",
      "| Reward: -1492 | Episode: 34 | Total step num: 6800 |\n",
      "| Reward: -890 | Episode: 35 | Total step num: 7000 |\n",
      "| Reward: -1600 | Episode: 36 | Total step num: 7200 |\n",
      "| Reward: -1344 | Episode: 37 | Total step num: 7400 |\n",
      "| Reward: -1064 | Episode: 38 | Total step num: 7600 |\n",
      "| Reward: -1073 | Episode: 39 | Total step num: 7800 |\n",
      "| Reward: -897 | Episode: 40 | Total step num: 8000 |\n",
      "| Reward: -1479 | Episode: 41 | Total step num: 8200 |\n",
      "| Reward: -1515 | Episode: 42 | Total step num: 8400 |\n",
      "| Reward: -1371 | Episode: 43 | Total step num: 8600 |\n",
      "| Reward: -636 | Episode: 44 | Total step num: 8800 |\n",
      "| Reward: -1050 | Episode: 45 | Total step num: 9000 |\n",
      "| Reward: -1788 | Episode: 46 | Total step num: 9200 |\n",
      "| Reward: -1295 | Episode: 47 | Total step num: 9400 |\n",
      "| Reward: -959 | Episode: 48 | Total step num: 9600 |\n",
      "| Reward: -1070 | Episode: 49 | Total step num: 9800 |\n",
      "total_step_cnt 10000\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1491.027\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1256.627\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1810.927\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1403.697\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1541.172\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1547.578\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1342.275\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1522.307\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1538.181\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1553.272\n",
      "---------------------------------------\n",
      "return_test[2] -1500\n",
      "| Reward: -1081 | Episode: 50 | Total step num: 10000 |\n",
      "| Reward: -1285 | Episode: 51 | Total step num: 10200 |\n",
      "| Reward: -1509 | Episode: 52 | Total step num: 10400 |\n",
      "| Reward: -1659 | Episode: 53 | Total step num: 10600 |\n",
      "| Reward: -1765 | Episode: 54 | Total step num: 10800 |\n",
      "| Reward: -1856 | Episode: 55 | Total step num: 11000 |\n",
      "| Reward: -1563 | Episode: 56 | Total step num: 11200 |\n",
      "| Reward: -1545 | Episode: 57 | Total step num: 11400 |\n",
      "| Reward: -1505 | Episode: 58 | Total step num: 11600 |\n",
      "| Reward: -1538 | Episode: 59 | Total step num: 11800 |\n",
      "| Reward: -1526 | Episode: 60 | Total step num: 12000 |\n",
      "| Reward: -1480 | Episode: 61 | Total step num: 12200 |\n",
      "| Reward: -1352 | Episode: 62 | Total step num: 12400 |\n",
      "| Reward: -855 | Episode: 63 | Total step num: 12600 |\n",
      "| Reward: -1155 | Episode: 64 | Total step num: 12800 |\n",
      "| Reward: -1110 | Episode: 65 | Total step num: 13000 |\n",
      "| Reward: -1262 | Episode: 66 | Total step num: 13200 |\n",
      "| Reward: -1178 | Episode: 67 | Total step num: 13400 |\n",
      "| Reward: -404 | Episode: 68 | Total step num: 13600 |\n",
      "| Reward: -932 | Episode: 69 | Total step num: 13800 |\n",
      "| Reward: -800 | Episode: 70 | Total step num: 14000 |\n",
      "| Reward: -823 | Episode: 71 | Total step num: 14200 |\n",
      "| Reward: -704 | Episode: 72 | Total step num: 14400 |\n",
      "| Reward: -264 | Episode: 73 | Total step num: 14600 |\n",
      "| Reward: -425 | Episode: 74 | Total step num: 14800 |\n",
      "total_step_cnt 15000\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -269.917\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -520.590\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -126.175\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -263.252\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -478.477\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -125.687\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -260.617\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1491.446\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -261.935\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -384.024\n",
      "---------------------------------------\n",
      "return_test[3] -418\n",
      "| Reward: -281 | Episode: 75 | Total step num: 15000 |\n",
      "| Reward: -268 | Episode: 76 | Total step num: 15200 |\n",
      "| Reward: -127 | Episode: 77 | Total step num: 15400 |\n",
      "| Reward: -121 | Episode: 78 | Total step num: 15600 |\n",
      "| Reward: -124 | Episode: 79 | Total step num: 15800 |\n",
      "| Reward: -130 | Episode: 80 | Total step num: 16000 |\n",
      "| Reward: -124 | Episode: 81 | Total step num: 16200 |\n",
      "| Reward: -1 | Episode: 82 | Total step num: 16400 |\n",
      "| Reward: -1 | Episode: 83 | Total step num: 16600 |\n",
      "| Reward: -288 | Episode: 84 | Total step num: 16800 |\n",
      "| Reward: -260 | Episode: 85 | Total step num: 17000 |\n",
      "| Reward: -125 | Episode: 86 | Total step num: 17200 |\n",
      "| Reward: -236 | Episode: 87 | Total step num: 17400 |\n",
      "| Reward: -2 | Episode: 88 | Total step num: 17600 |\n",
      "| Reward: -122 | Episode: 89 | Total step num: 17800 |\n",
      "| Reward: -115 | Episode: 90 | Total step num: 18000 |\n",
      "| Reward: -355 | Episode: 91 | Total step num: 18200 |\n",
      "| Reward: -125 | Episode: 92 | Total step num: 18400 |\n",
      "| Reward: -245 | Episode: 93 | Total step num: 18600 |\n",
      "| Reward: -127 | Episode: 94 | Total step num: 18800 |\n",
      "| Reward: -342 | Episode: 95 | Total step num: 19000 |\n",
      "| Reward: -1 | Episode: 96 | Total step num: 19200 |\n",
      "| Reward: -127 | Episode: 97 | Total step num: 19400 |\n",
      "| Reward: -230 | Episode: 98 | Total step num: 19600 |\n",
      "| Reward: -126 | Episode: 99 | Total step num: 19800 |\n",
      "total_step_cnt 20000\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -329.001\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -120.838\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -120.301\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -226.832\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -117.775\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -117.455\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -231.131\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -119.370\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -347.625\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -120.891\n",
      "---------------------------------------\n",
      "return_test[4] -185\n",
      "| Reward: -286 | Episode: 100 | Total step num: 20000 |\n",
      "| Reward: -125 | Episode: 101 | Total step num: 20200 |\n",
      "| Reward: -118 | Episode: 102 | Total step num: 20400 |\n",
      "| Reward: -241 | Episode: 103 | Total step num: 20600 |\n",
      "| Reward: -124 | Episode: 104 | Total step num: 20800 |\n",
      "| Reward: -121 | Episode: 105 | Total step num: 21000 |\n",
      "| Reward: 0 | Episode: 106 | Total step num: 21200 |\n",
      "| Reward: -225 | Episode: 107 | Total step num: 21400 |\n",
      "| Reward: -331 | Episode: 108 | Total step num: 21600 |\n",
      "| Reward: -235 | Episode: 109 | Total step num: 21800 |\n",
      "| Reward: -115 | Episode: 110 | Total step num: 22000 |\n",
      "| Reward: -116 | Episode: 111 | Total step num: 22200 |\n",
      "| Reward: -236 | Episode: 112 | Total step num: 22400 |\n",
      "| Reward: -234 | Episode: 113 | Total step num: 22600 |\n",
      "| Reward: -242 | Episode: 114 | Total step num: 22800 |\n",
      "| Reward: -121 | Episode: 115 | Total step num: 23000 |\n",
      "| Reward: -1 | Episode: 116 | Total step num: 23200 |\n",
      "| Reward: -127 | Episode: 117 | Total step num: 23400 |\n",
      "| Reward: -122 | Episode: 118 | Total step num: 23600 |\n",
      "| Reward: -1 | Episode: 119 | Total step num: 23800 |\n",
      "| Reward: 0 | Episode: 120 | Total step num: 24000 |\n",
      "| Reward: -127 | Episode: 121 | Total step num: 24200 |\n",
      "| Reward: -234 | Episode: 122 | Total step num: 24400 |\n",
      "| Reward: -115 | Episode: 123 | Total step num: 24600 |\n",
      "| Reward: -120 | Episode: 124 | Total step num: 24800 |\n",
      "total_step_cnt 25000\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -119.888\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -119.072\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -234.564\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -116.678\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -117.568\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -237.820\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -119.127\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -229.663\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -118.781\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -342.487\n",
      "---------------------------------------\n",
      "return_test[5] -175\n",
      "| Reward: -117 | Episode: 125 | Total step num: 25000 |\n",
      "The result of the trial no.1 was saved.\n",
      "Trial Number: 2\n",
      "action_space.shape (1,)\n",
      "observation_space.shape (3,)\n",
      "total_step_cnt 1\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1347.928\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1159.496\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1196.689\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1779.724\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1310.271\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1185.177\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1589.212\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1235.048\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1200.899\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1586.512\n",
      "---------------------------------------\n",
      "return_test[0] -1359\n",
      "| Reward: -985 | Episode: 1 | Total step num: 200 |\n",
      "| Reward: -796 | Episode: 2 | Total step num: 400 |\n",
      "| Reward: -1756 | Episode: 3 | Total step num: 600 |\n",
      "| Reward: -754 | Episode: 4 | Total step num: 800 |\n",
      "| Reward: -1678 | Episode: 5 | Total step num: 1000 |\n",
      "| Reward: -1353 | Episode: 6 | Total step num: 1200 |\n",
      "| Reward: -1241 | Episode: 7 | Total step num: 1400 |\n",
      "| Reward: -1480 | Episode: 8 | Total step num: 1600 |\n",
      "| Reward: -1618 | Episode: 9 | Total step num: 1800 |\n",
      "| Reward: -1367 | Episode: 10 | Total step num: 2000 |\n",
      "| Reward: -1171 | Episode: 11 | Total step num: 2200 |\n",
      "| Reward: -1237 | Episode: 12 | Total step num: 2400 |\n",
      "| Reward: -1317 | Episode: 13 | Total step num: 2600 |\n",
      "| Reward: -1700 | Episode: 14 | Total step num: 2800 |\n",
      "| Reward: -951 | Episode: 15 | Total step num: 3000 |\n",
      "| Reward: -1245 | Episode: 16 | Total step num: 3200 |\n",
      "| Reward: -1692 | Episode: 17 | Total step num: 3400 |\n",
      "| Reward: -957 | Episode: 18 | Total step num: 3600 |\n",
      "| Reward: -1253 | Episode: 19 | Total step num: 3800 |\n",
      "| Reward: -1778 | Episode: 20 | Total step num: 4000 |\n",
      "| Reward: -891 | Episode: 21 | Total step num: 4200 |\n",
      "| Reward: -1655 | Episode: 22 | Total step num: 4400 |\n",
      "| Reward: -1127 | Episode: 23 | Total step num: 4600 |\n",
      "| Reward: -1750 | Episode: 24 | Total step num: 4800 |\n",
      "total_step_cnt 5000\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1159.496\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1196.689\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1779.724\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1310.271\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1185.177\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1589.212\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1235.048\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1200.899\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1586.512\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1192.425\n",
      "---------------------------------------\n",
      "return_test[1] -1343\n",
      "| Reward: -1015 | Episode: 25 | Total step num: 5000 |\n",
      "| Reward: -1169 | Episode: 26 | Total step num: 5200 |\n",
      "| Reward: -1778 | Episode: 27 | Total step num: 5400 |\n",
      "| Reward: -1079 | Episode: 28 | Total step num: 5600 |\n",
      "| Reward: -1595 | Episode: 29 | Total step num: 5800 |\n",
      "| Reward: -754 | Episode: 30 | Total step num: 6000 |\n",
      "| Reward: -1183 | Episode: 31 | Total step num: 6200 |\n",
      "| Reward: -1763 | Episode: 32 | Total step num: 6400 |\n",
      "| Reward: -1573 | Episode: 33 | Total step num: 6600 |\n",
      "| Reward: -1074 | Episode: 34 | Total step num: 6800 |\n",
      "| Reward: -1364 | Episode: 35 | Total step num: 7000 |\n",
      "| Reward: -1362 | Episode: 36 | Total step num: 7200 |\n",
      "| Reward: -1016 | Episode: 37 | Total step num: 7400 |\n",
      "| Reward: -861 | Episode: 38 | Total step num: 7600 |\n",
      "| Reward: -812 | Episode: 39 | Total step num: 7800 |\n",
      "| Reward: -1489 | Episode: 40 | Total step num: 8000 |\n",
      "| Reward: -1690 | Episode: 41 | Total step num: 8200 |\n",
      "| Reward: -1347 | Episode: 42 | Total step num: 8400 |\n",
      "| Reward: -854 | Episode: 43 | Total step num: 8600 |\n",
      "| Reward: -1029 | Episode: 44 | Total step num: 8800 |\n",
      "| Reward: -1682 | Episode: 45 | Total step num: 9000 |\n",
      "| Reward: -1364 | Episode: 46 | Total step num: 9200 |\n",
      "| Reward: -1239 | Episode: 47 | Total step num: 9400 |\n",
      "| Reward: -770 | Episode: 48 | Total step num: 9600 |\n",
      "| Reward: -1233 | Episode: 49 | Total step num: 9800 |\n",
      "total_step_cnt 10000\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1196.689\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1779.724\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1310.271\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1185.177\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1589.212\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1235.048\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1200.899\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1586.512\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1192.425\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1590.866\n",
      "---------------------------------------\n",
      "return_test[2] -1386\n",
      "| Reward: -1166 | Episode: 50 | Total step num: 10000 |\n",
      "| Reward: -1351 | Episode: 51 | Total step num: 10200 |\n",
      "| Reward: -1478 | Episode: 52 | Total step num: 10400 |\n",
      "| Reward: -1727 | Episode: 53 | Total step num: 10600 |\n",
      "| Reward: -1829 | Episode: 54 | Total step num: 10800 |\n",
      "| Reward: -1536 | Episode: 55 | Total step num: 11000 |\n",
      "| Reward: -1497 | Episode: 56 | Total step num: 11200 |\n",
      "| Reward: -1474 | Episode: 57 | Total step num: 11400 |\n",
      "| Reward: -1520 | Episode: 58 | Total step num: 11600 |\n",
      "| Reward: -1529 | Episode: 59 | Total step num: 11800 |\n",
      "| Reward: -1467 | Episode: 60 | Total step num: 12000 |\n",
      "| Reward: -1388 | Episode: 61 | Total step num: 12200 |\n",
      "| Reward: -300 | Episode: 62 | Total step num: 12400 |\n",
      "| Reward: -1193 | Episode: 63 | Total step num: 12600 |\n",
      "| Reward: -1148 | Episode: 64 | Total step num: 12800 |\n",
      "| Reward: -1515 | Episode: 65 | Total step num: 13000 |\n",
      "| Reward: -1512 | Episode: 66 | Total step num: 13200 |\n",
      "| Reward: -513 | Episode: 67 | Total step num: 13400 |\n",
      "| Reward: -922 | Episode: 68 | Total step num: 13600 |\n",
      "| Reward: -920 | Episode: 69 | Total step num: 13800 |\n",
      "| Reward: -946 | Episode: 70 | Total step num: 14000 |\n",
      "| Reward: -1520 | Episode: 71 | Total step num: 14200 |\n",
      "| Reward: -261 | Episode: 72 | Total step num: 14400 |\n",
      "| Reward: -299 | Episode: 73 | Total step num: 14600 |\n",
      "| Reward: -410 | Episode: 74 | Total step num: 14800 |\n",
      "total_step_cnt 15000\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -387.198\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -125.963\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -130.695\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -460.685\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -125.564\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -128.609\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1491.607\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -126.972\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -377.887\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -130.810\n",
      "---------------------------------------\n",
      "return_test[3] -348\n",
      "| Reward: -269 | Episode: 75 | Total step num: 15000 |\n",
      "| Reward: -127 | Episode: 76 | Total step num: 15200 |\n",
      "| Reward: -120 | Episode: 77 | Total step num: 15400 |\n",
      "| Reward: -257 | Episode: 78 | Total step num: 15600 |\n",
      "| Reward: -129 | Episode: 79 | Total step num: 15800 |\n",
      "| Reward: -123 | Episode: 80 | Total step num: 16000 |\n",
      "| Reward: 0 | Episode: 81 | Total step num: 16200 |\n",
      "| Reward: 0 | Episode: 82 | Total step num: 16400 |\n",
      "| Reward: -291 | Episode: 83 | Total step num: 16600 |\n",
      "| Reward: -264 | Episode: 84 | Total step num: 16800 |\n",
      "| Reward: -125 | Episode: 85 | Total step num: 17000 |\n",
      "| Reward: -236 | Episode: 86 | Total step num: 17200 |\n",
      "| Reward: 0 | Episode: 87 | Total step num: 17400 |\n",
      "| Reward: -122 | Episode: 88 | Total step num: 17600 |\n",
      "| Reward: -116 | Episode: 89 | Total step num: 17800 |\n",
      "| Reward: -357 | Episode: 90 | Total step num: 18000 |\n",
      "| Reward: -124 | Episode: 91 | Total step num: 18200 |\n",
      "| Reward: -246 | Episode: 92 | Total step num: 18400 |\n",
      "| Reward: -258 | Episode: 93 | Total step num: 18600 |\n",
      "| Reward: -346 | Episode: 94 | Total step num: 18800 |\n",
      "| Reward: 0 | Episode: 95 | Total step num: 19000 |\n",
      "| Reward: -126 | Episode: 96 | Total step num: 19200 |\n",
      "| Reward: -231 | Episode: 97 | Total step num: 19400 |\n",
      "| Reward: -387 | Episode: 98 | Total step num: 19600 |\n",
      "| Reward: -295 | Episode: 99 | Total step num: 19800 |\n",
      "total_step_cnt 20000\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -120.482\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1046.507\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -227.549\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -116.723\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -986.581\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -231.474\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1051.123\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -222.253\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -122.108\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -322.966\n",
      "---------------------------------------\n",
      "return_test[4] -444\n",
      "| Reward: -128 | Episode: 100 | Total step num: 20000 |\n",
      "| Reward: -383 | Episode: 101 | Total step num: 20200 |\n",
      "| Reward: -844 | Episode: 102 | Total step num: 20400 |\n",
      "| Reward: -125 | Episode: 103 | Total step num: 20600 |\n",
      "| Reward: -121 | Episode: 104 | Total step num: 20800 |\n",
      "| Reward: -1 | Episode: 105 | Total step num: 21000 |\n",
      "| Reward: -229 | Episode: 106 | Total step num: 21200 |\n",
      "| Reward: -332 | Episode: 107 | Total step num: 21400 |\n",
      "| Reward: -237 | Episode: 108 | Total step num: 21600 |\n",
      "| Reward: -116 | Episode: 109 | Total step num: 21800 |\n",
      "| Reward: -116 | Episode: 110 | Total step num: 22000 |\n",
      "| Reward: -234 | Episode: 111 | Total step num: 22200 |\n",
      "| Reward: -233 | Episode: 112 | Total step num: 22400 |\n",
      "| Reward: -243 | Episode: 113 | Total step num: 22600 |\n",
      "| Reward: -119 | Episode: 114 | Total step num: 22800 |\n",
      "| Reward: -1 | Episode: 115 | Total step num: 23000 |\n",
      "| Reward: -127 | Episode: 116 | Total step num: 23200 |\n",
      "| Reward: -122 | Episode: 117 | Total step num: 23400 |\n",
      "| Reward: -2 | Episode: 118 | Total step num: 23600 |\n",
      "| Reward: 0 | Episode: 119 | Total step num: 23800 |\n",
      "| Reward: -127 | Episode: 120 | Total step num: 24000 |\n",
      "| Reward: -234 | Episode: 121 | Total step num: 24200 |\n",
      "| Reward: -115 | Episode: 122 | Total step num: 24400 |\n",
      "| Reward: -120 | Episode: 123 | Total step num: 24600 |\n",
      "| Reward: -117 | Episode: 124 | Total step num: 24800 |\n",
      "total_step_cnt 25000\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -119.013\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -235.342\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -116.578\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -117.663\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -240.031\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -119.301\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -230.813\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -118.752\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -343.235\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1.206\n",
      "---------------------------------------\n",
      "return_test[5] -164\n",
      "| Reward: -330 | Episode: 125 | Total step num: 25000 |\n",
      "The result of the trial no.2 was saved.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "def set_all_seeds(seed=1234):\n",
    "    \"\"\"set all random seed\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    import os\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# 在程序开始时设置种子\n",
    "set_all_seeds(1234)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA not available, using CPU\")\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# run parameters\n",
    "parser.add_argument('--env', help='choose the gym env- tested on {Pendulum-v1}')\n",
    "parser.add_argument('--env-id', type=int, default=0, help='choose the gym env- tested on {Pendulum-v1}')\n",
    "parser.add_argument('--random-seed', help='random seed for repeatability', default=1234)\n",
    "parser.add_argument('--max-episodes', help='max num of episodes to do while training', default=1001)\n",
    "parser.add_argument('--max-episode-len', help='max length of 1 episode', default=1000)\n",
    "parser.add_argument('--trial-num', help='number of trials', default=3)\n",
    "# parser.add_argument('--render-env', help='render the gym env', action='store_true')\n",
    "parser.add_argument('--total-step-num', help='total number of time steps', default=25000)\n",
    "parser.add_argument('--eval-step-freq', help='frequency of evaluating the policy', default=5000)\n",
    "parser.add_argument('--test-num', help='number of test episodes', default=10)\n",
    "\n",
    "parser.add_argument('--result-file', help='file name for storing results from multiple trials',\n",
    "                    default='./results/trials/td3/trials_td3_')\n",
    "parser.add_argument('--trial-idx', help='index of trials', default=0)\n",
    "parser.add_argument('--monitor-dir', help='directory for recording', default='results/video/td3')\n",
    "\n",
    "parser.add_argument(\"--start_timesteps\", default=1e4, type=int)  # How many time steps purely random policy is run for\n",
    "parser.add_argument(\"--expl_noise\", default=0.1, type=float)  # Std of Gaussian exploration noise\n",
    "parser.add_argument(\"--batch_size\", default=256, type=int)  # Batch size for both actor and critic\n",
    "parser.add_argument(\"--update_freq\", default=1, type=int)  # Number of policy updates\n",
    "\n",
    "parser.set_defaults(render_env=False)\n",
    "\n",
    "args_tmp, unknown = parser.parse_known_args()\n",
    "\n",
    "if args_tmp.env is None:\n",
    "    env_dict = {0 : \"Pendulum-v1\",\n",
    "    }\n",
    "    args_tmp.env = env_dict[args_tmp.env_id]\n",
    "args = vars(args_tmp)\n",
    "\n",
    "return_set=[]\n",
    "for ite in range(int(args['trial_num'])):\n",
    "    print('Trial Number:', ite)\n",
    "\n",
    "    seed = 1234 + ite            \n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    index = int(ite) + int(args['trial_idx'])\n",
    "    env = gym.make(args['env'])\n",
    "    env.reset(seed=seed)\n",
    "    env.action_space.seed(seed)  # 有时 action_space 也需要显式 seed\n",
    "\n",
    "    print('action_space.shape', env.action_space.shape)\n",
    "    print('observation_space.shape', env.observation_space.shape)\n",
    "    action_bound = float(env.action_space.high[0])\n",
    "\n",
    "    assert (env.action_space.high[0] == -env.action_space.low[0])\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "\n",
    "    agent = TD3(state_dim=state_dim, action_dim=action_dim, max_action=action_bound)\n",
    "\n",
    "    step_R_i = train(env, agent, args, index, seed)\n",
    "    return_set.append(step_R_i)\n",
    "\n",
    "    result_path = \"./results/trials/td3\"\n",
    "    result_filename = args['result_file'] +  \\\n",
    "                      '_update_freq_' + str(int(args['update_freq'])) + '_' + args['env'] +  \\\n",
    "                      '_trial_idx_' + str(index) + '.txt'\n",
    "    try:\n",
    "        import pathlib\n",
    "        pathlib.Path(result_path).mkdir(parents=True, exist_ok=True)\n",
    "        np.savetxt(result_filename, np.asarray(step_R_i))\n",
    "        print('The result of the trial no.' + str(index) + ' was saved.')\n",
    "    except:\n",
    "        print(\"A result directory does not exist and cannot be created. The trial results are not saved\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdSCbpbYhfME"
   },
   "source": [
    "We plot the results of the training using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "WjNBeCJPhdQY",
    "outputId": "d615d5a2-1125-4569-c339-caf1ca0cb940"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "figure(figsize=(7, 6))\n",
    "\n",
    "t = np.arange(0, int(args['total_step_num']) + 1, int(args['eval_step_freq'])) * 0.001\n",
    "\n",
    "mean = np.mean(np.asarray(return_set), axis=0)\n",
    "std = np.std(np.asarray(return_set), axis=0)\n",
    "color = 'b'\n",
    "label = 'TD3'\n",
    "plt.plot(t, mean, color, label=label)\n",
    "plt.fill(np.concatenate([t, t[::-1]]), np.concatenate([mean - 1.9600 * std,\n",
    "                                      (mean + 1.9600 * std)[::-1]]), alpha=.1, fc=color, ec='None')\n",
    "\n",
    "plt.xlabel('Time steps [x 1e3]', fontsize=14)\n",
    "plt.ylabel('Return', fontsize=14)\n",
    "plt.legend(loc='lower right', fontsize=14)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    # changed: add log_std_min, log_std_max\n",
    "    def __init__(self, state_dim, action_dim, max_action, log_std_min=-20, log_std_max=2):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        # changed: SAC ：mean and log_std\n",
    "        self.mean = nn.Linear(256, action_dim)\n",
    "        self.log_std = nn.Linear(256, action_dim)\n",
    "\n",
    "        # changed: record log_std upper & lower bounds\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.l1(state))\n",
    "        x = F.relu(self.l2(x))\n",
    "        mean = self.mean(x)\n",
    "        log_std = self.log_std(x)\n",
    "        # changed: max\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        return mean, log_std\n",
    "\n",
    "    # changed: SAC reparameterization and calculate log_pi\n",
    "    def sample(self, state):\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "\n",
    "       \n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        x_t = normal.rsample()\n",
    "        log_pi = normal.log_prob(x_t).sum(dim=-1, keepdim=True)\n",
    "\n",
    "    \n",
    "        action = torch.tanh(x_t)\n",
    "\n",
    "        log_pi -= torch.log(1 - action.pow(2) + 1e-6).sum(dim=-1, keepdim=True)\n",
    "\n",
    "        # [-max_action, max_action]\n",
    "        action = self.max_action * action\n",
    "        return action, log_pi, mean, log_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic the same as TD3\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        # Q1 architecture\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, 1)\n",
    "\n",
    "        # Q2 architecture\n",
    "        self.l4 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.l5 = nn.Linear(256, 256)\n",
    "        self.l6 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        sa = torch.cat([state, action], dim=1)\n",
    "\n",
    "        q1 = F.relu(self.l1(sa))\n",
    "        q1 = F.relu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "\n",
    "        q2 = F.relu(self.l4(sa))\n",
    "        q2 = F.relu(self.l5(q2))\n",
    "        q2 = self.l6(q2)\n",
    "        return q1, q2\n",
    "\n",
    "    def Q1(self, state, action):\n",
    "        sa = torch.cat([state, action], dim=1)\n",
    "        q1 = F.relu(self.l1(sa))\n",
    "        q1 = F.relu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "        return q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC(object):\n",
    "    # changed: delete policy_noise, noise_clip, policy_freq\n",
    "    #          add alpha\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        max_action,\n",
    "        discount=0.7,\n",
    "        tau=0.015,\n",
    "        alpha=0.2,            # changed: initial alpha\n",
    "        target_entropy=None,  \n",
    "        automatic_entropy_tuning=True  # changed: adjust alpha\n",
    "    ):\n",
    "\n",
    "        # changed: SAC Actor\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "\n",
    "        # Critic, target critic\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target = copy.deepcopy(self.critic)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "\n",
    "        self.max_action = max_action\n",
    "        self.discount = discount\n",
    "        self.tau = tau\n",
    "\n",
    "        # changed: adjust alpha\n",
    "        self.automatic_entropy_tuning = automatic_entropy_tuning\n",
    "        if target_entropy is None:\n",
    "            self.target_entropy = -action_dim  # usually -action_dim\n",
    "        else:\n",
    "            self.target_entropy = target_entropy\n",
    "\n",
    "        if self.automatic_entropy_tuning:\n",
    "            # changed: log_alpha\n",
    "            self.log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "            self.alpha_optimizer = torch.optim.Adam([self.log_alpha], lr=3e-4)\n",
    "        else:\n",
    "            # changed: fix alpha\n",
    "            self.log_alpha = torch.tensor(np.log(alpha)).to(device)\n",
    "            self.alpha_optimizer = None\n",
    "\n",
    "        self.total_it = 0\n",
    "\n",
    "    # changed: SAC' select_action\n",
    "    def select_action(self, state, test=False):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        if test:\n",
    "            # test mean\n",
    "            mean, log_std = self.actor(state)\n",
    "            action = torch.tanh(mean) * self.max_action\n",
    "            return action.cpu().data.numpy().flatten()\n",
    "        else:\n",
    "            # train random\n",
    "            action, _, _, _ = self.actor.sample(state)\n",
    "            return action.cpu().data.numpy().flatten()\n",
    "\n",
    "\n",
    "    def train(self, replay_buffer, batch_size=256):\n",
    "        self.total_it += 1\n",
    "\n",
    "        state, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_action, next_log_pi, _, _ = self.actor.sample(next_state)\n",
    "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2) - self.alpha * next_log_pi\n",
    "            target_Q = reward + not_done * self.discount * target_Q\n",
    "\n",
    "        current_Q1, current_Q2 = self.critic(state, action)\n",
    "        critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        a, log_pi, _, _ = self.actor.sample(state)\n",
    "        Q1, Q2 = self.critic(state, a)\n",
    "        Q = torch.min(Q1, Q2)\n",
    "        actor_loss = (self.alpha * log_pi - Q).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        if self.automatic_entropy_tuning:\n",
    "            alpha_loss = (-(self.log_alpha * (log_pi + self.target_entropy).detach())).mean()\n",
    "            self.alpha_optimizer.zero_grad()\n",
    "            alpha_loss.backward()\n",
    "            self.alpha_optimizer.step()\n",
    "\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                self.tau * param.data + (1 - self.tau) * target_param.data\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def alpha(self):\n",
    "        return self.log_alpha.exp()\n",
    "\n",
    "    def save(self, filename):\n",
    "        torch.save(self.critic.state_dict(), filename + \"_critic\")\n",
    "        torch.save(self.critic_optimizer.state_dict(), filename + \"_critic_optimizer\")\n",
    "        torch.save(self.actor.state_dict(), filename + \"_actor\")\n",
    "        torch.save(self.actor_optimizer.state_dict(), filename + \"_actor_optimizer\")\n",
    "        if self.automatic_entropy_tuning:\n",
    "            torch.save(self.log_alpha, filename + \"_log_alpha\")\n",
    "            torch.save(self.alpha_optimizer.state_dict(), filename + \"_alpha_optimizer\")\n",
    "\n",
    "    def load(self, filename):\n",
    "        self.critic.load_state_dict(torch.load(filename + \"_critic\"))\n",
    "        self.critic_optimizer.load_state_dict(torch.load(filename + \"_critic_optimizer\"))\n",
    "        self.actor.load_state_dict(torch.load(filename + \"_actor\"))\n",
    "        self.actor_optimizer.load_state_dict(torch.load(filename + \"_actor_optimizer\"))\n",
    "        if self.automatic_entropy_tuning:\n",
    "            self.log_alpha = torch.load(filename + \"_log_alpha\")\n",
    "            self.alpha_optimizer.load_state_dict(torch.load(filename + \"_alpha_optimizer\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "\n",
    "        self.state = np.zeros((max_size, state_dim))\n",
    "        self.action = np.zeros((max_size, action_dim))\n",
    "        self.next_state = np.zeros((max_size, state_dim))\n",
    "        self.reward = np.zeros((max_size, 1))\n",
    "        self.not_done = np.zeros((max_size, 1))\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def add(self, state, action, next_state, reward, done):\n",
    "        self.state[self.ptr] = state\n",
    "        self.action[self.ptr] = action\n",
    "        self.next_state[self.ptr] = next_state\n",
    "        self.reward[self.ptr] = reward\n",
    "        self.not_done[self.ptr] = 1. - done\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(self.size, size=batch_size)\n",
    "\n",
    "        return (\n",
    "            torch.FloatTensor(self.state[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.action[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.next_state[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.reward[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.not_done[ind]).to(self.device)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "def eval_policy(policy, env, seed, eval_episodes=10):\n",
    "    eval_env = gym.make(env)\n",
    "\n",
    "    avg_reward = 0.\n",
    "    for _ in range(eval_episodes):\n",
    "        state, info = eval_env.reset(seed=seed+eval_episodes)        \n",
    "        eval_env.action_space.seed(seed+eval_episodes)\n",
    "        terminated, truncated = False, False\n",
    "        while not (terminated or truncated):\n",
    "            action = policy.select_action(np.array(state), test=True)\n",
    "            state, reward, terminated, truncated, info  = eval_env.step(action)\n",
    "            avg_reward += reward\n",
    "\n",
    "    avg_reward /= eval_episodes\n",
    "\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "    print(\"---------------------------------------\")\n",
    "    return avg_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, args, index, trial_seed):\n",
    "\n",
    "    # Initialize replay memory\n",
    "    total_step_cnt = 0\n",
    "    epi_cnt = 0\n",
    "    test_iter = 0\n",
    "    return_test = np.zeros((np.ceil(int(args['total_step_num']) / int(args['eval_step_freq'])).astype('int') + 1))\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    max_action = float(env.action_space.high[0])\n",
    "\n",
    "    replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
    "    episode_timesteps = 0\n",
    "\n",
    "    while total_step_cnt in range(int(args['total_step_num'])):\n",
    "\n",
    "        state, info = env.reset(seed=trial_seed + epi_cnt)\n",
    "        ep_reward = 0\n",
    "\n",
    "        for t in range(int(args['max_episode_len'])):\n",
    "\n",
    "            # Select action randomly or according to policy\n",
    "            if total_step_cnt < int(args['start_timesteps']):\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = agent.select_action(np.array(state))\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done_bool = float(terminated) if episode_timesteps < env._max_episode_steps else 0\n",
    "\n",
    "            replay_buffer.add(state, action, next_state, reward, done_bool)\n",
    "\n",
    "            if total_step_cnt >= int(args['start_timesteps']):\n",
    "                for i in range(int(args['update_freq'])):\n",
    "                    agent.train(replay_buffer, int(args['batch_size']))\n",
    "\n",
    "            state = next_state\n",
    "            ep_reward += reward\n",
    "            total_step_cnt += 1\n",
    "\n",
    "            # Evaluate the deterministic policy\n",
    "            if total_step_cnt >= test_iter * int(args['eval_step_freq']) or total_step_cnt == 1:\n",
    "                print('total_step_cnt', total_step_cnt)\n",
    "                print('evaluating the deterministic policy...')\n",
    "                for test_n in range(int(args['test_num'])):\n",
    "                    return_epi_test = eval_policy(agent, args['env'], trial_seed+test_n+test_iter, eval_episodes=args['test_num'])\n",
    "\n",
    "                    return_test[test_iter] += return_epi_test / float(args['test_num'])\n",
    "\n",
    "                print('return_test[{:d}] {:d}'.format(int(test_iter), int(return_test[test_iter])))\n",
    "                test_iter += 1\n",
    "\n",
    "            if terminated or truncated:\n",
    "                epi_cnt += 1\n",
    "                print('| Reward: {:d} | Episode: {:d} | Total step num: {:d} |'.format(\n",
    "                    int(ep_reward), epi_cnt, total_step_cnt\n",
    "                ))\n",
    "                break\n",
    "\n",
    "    return return_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device: NVIDIA GeForce RTX 3090\n",
      "Trial Number: 0\n",
      "action_space.shape (1,)\n",
      "observation_space.shape (3,)\n",
      "total_step_cnt 1\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1382.448\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1280.658\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -976.828\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -978.300\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1336.772\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1860.920\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -988.357\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1054.857\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1725.251\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1007.191\n",
      "---------------------------------------\n",
      "return_test[0] -1259\n",
      "| Reward: -1877 | Episode: 1 | Total step num: 200 |\n",
      "| Reward: -1489 | Episode: 2 | Total step num: 400 |\n",
      "| Reward: -943 | Episode: 3 | Total step num: 600 |\n",
      "| Reward: -860 | Episode: 4 | Total step num: 800 |\n",
      "| Reward: -1845 | Episode: 5 | Total step num: 1000 |\n",
      "| Reward: -1063 | Episode: 6 | Total step num: 1200 |\n",
      "| Reward: -1723 | Episode: 7 | Total step num: 1400 |\n",
      "| Reward: -1184 | Episode: 8 | Total step num: 1600 |\n",
      "| Reward: -1293 | Episode: 9 | Total step num: 1800 |\n",
      "| Reward: -1329 | Episode: 10 | Total step num: 2000 |\n",
      "| Reward: -1302 | Episode: 11 | Total step num: 2200 |\n",
      "| Reward: -1271 | Episode: 12 | Total step num: 2400 |\n",
      "| Reward: -795 | Episode: 13 | Total step num: 2600 |\n",
      "| Reward: -1166 | Episode: 14 | Total step num: 2800 |\n",
      "| Reward: -1457 | Episode: 15 | Total step num: 3000 |\n",
      "| Reward: -1753 | Episode: 16 | Total step num: 3200 |\n",
      "| Reward: -1160 | Episode: 17 | Total step num: 3400 |\n",
      "| Reward: -971 | Episode: 18 | Total step num: 3600 |\n",
      "| Reward: -1601 | Episode: 19 | Total step num: 3800 |\n",
      "| Reward: -1000 | Episode: 20 | Total step num: 4000 |\n",
      "| Reward: -1269 | Episode: 21 | Total step num: 4200 |\n",
      "| Reward: -1585 | Episode: 22 | Total step num: 4400 |\n",
      "| Reward: -1054 | Episode: 23 | Total step num: 4600 |\n",
      "| Reward: -1501 | Episode: 24 | Total step num: 4800 |\n",
      "total_step_cnt 5000\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1280.658\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -976.828\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -978.300\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1336.772\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1860.920\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -988.357\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1054.857\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1725.251\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1007.191\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1011.135\n",
      "---------------------------------------\n",
      "return_test[1] -1222\n",
      "| Reward: -768 | Episode: 25 | Total step num: 5000 |\n",
      "| Reward: -1811 | Episode: 26 | Total step num: 5200 |\n",
      "| Reward: -911 | Episode: 27 | Total step num: 5400 |\n",
      "| Reward: -1110 | Episode: 28 | Total step num: 5600 |\n",
      "| Reward: -1687 | Episode: 29 | Total step num: 5800 |\n",
      "| Reward: -909 | Episode: 30 | Total step num: 6000 |\n",
      "| Reward: -1658 | Episode: 31 | Total step num: 6200 |\n",
      "| Reward: -931 | Episode: 32 | Total step num: 6400 |\n",
      "| Reward: -1139 | Episode: 33 | Total step num: 6600 |\n",
      "| Reward: -1635 | Episode: 34 | Total step num: 6800 |\n",
      "| Reward: -1708 | Episode: 35 | Total step num: 7000 |\n",
      "| Reward: -895 | Episode: 36 | Total step num: 7200 |\n",
      "| Reward: -1623 | Episode: 37 | Total step num: 7400 |\n",
      "| Reward: -1373 | Episode: 38 | Total step num: 7600 |\n",
      "| Reward: -923 | Episode: 39 | Total step num: 7800 |\n",
      "| Reward: -1286 | Episode: 40 | Total step num: 8000 |\n",
      "| Reward: -928 | Episode: 41 | Total step num: 8200 |\n",
      "| Reward: -1455 | Episode: 42 | Total step num: 8400 |\n",
      "| Reward: -1674 | Episode: 43 | Total step num: 8600 |\n",
      "| Reward: -1193 | Episode: 44 | Total step num: 8800 |\n",
      "| Reward: -870 | Episode: 45 | Total step num: 9000 |\n",
      "| Reward: -1156 | Episode: 46 | Total step num: 9200 |\n",
      "| Reward: -1763 | Episode: 47 | Total step num: 9400 |\n",
      "| Reward: -1413 | Episode: 48 | Total step num: 9600 |\n",
      "| Reward: -969 | Episode: 49 | Total step num: 9800 |\n",
      "total_step_cnt 10000\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -976.828\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -978.300\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1336.772\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1860.920\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -988.357\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1054.857\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1725.251\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1007.191\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1011.135\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1719.378\n",
      "---------------------------------------\n",
      "return_test[2] -1265\n",
      "| Reward: -863 | Episode: 50 | Total step num: 10000 |\n",
      "| Reward: -1056 | Episode: 51 | Total step num: 10200 |\n",
      "| Reward: -1458 | Episode: 52 | Total step num: 10400 |\n",
      "| Reward: -1629 | Episode: 53 | Total step num: 10600 |\n",
      "| Reward: -1216 | Episode: 54 | Total step num: 10800 |\n",
      "| Reward: -1318 | Episode: 55 | Total step num: 11000 |\n",
      "| Reward: -1597 | Episode: 56 | Total step num: 11200 |\n",
      "| Reward: -1246 | Episode: 57 | Total step num: 11400 |\n",
      "| Reward: -1250 | Episode: 58 | Total step num: 11600 |\n",
      "| Reward: -1131 | Episode: 59 | Total step num: 11800 |\n",
      "| Reward: -1189 | Episode: 60 | Total step num: 12000 |\n",
      "| Reward: -1115 | Episode: 61 | Total step num: 12200 |\n",
      "| Reward: -858 | Episode: 62 | Total step num: 12400 |\n",
      "| Reward: -752 | Episode: 63 | Total step num: 12600 |\n",
      "| Reward: -1 | Episode: 64 | Total step num: 12800 |\n",
      "| Reward: -126 | Episode: 65 | Total step num: 13000 |\n",
      "| Reward: -131 | Episode: 66 | Total step num: 13200 |\n",
      "| Reward: -419 | Episode: 67 | Total step num: 13400 |\n",
      "| Reward: -462 | Episode: 68 | Total step num: 13600 |\n",
      "| Reward: 0 | Episode: 69 | Total step num: 13800 |\n",
      "| Reward: -126 | Episode: 70 | Total step num: 14000 |\n",
      "| Reward: -124 | Episode: 71 | Total step num: 14200 |\n",
      "| Reward: -339 | Episode: 72 | Total step num: 14400 |\n",
      "| Reward: -236 | Episode: 73 | Total step num: 14600 |\n",
      "| Reward: -119 | Episode: 74 | Total step num: 14800 |\n",
      "total_step_cnt 15000\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -117.591\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -244.356\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -341.594\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -119.940\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -122.126\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -232.822\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -117.193\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -119.553\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -236.571\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -120.661\n",
      "---------------------------------------\n",
      "return_test[3] -177\n",
      "| Reward: -251 | Episode: 75 | Total step num: 15000 |\n",
      "| Reward: -244 | Episode: 76 | Total step num: 15200 |\n",
      "| Reward: -124 | Episode: 77 | Total step num: 15400 |\n",
      "| Reward: -118 | Episode: 78 | Total step num: 15600 |\n",
      "| Reward: -115 | Episode: 79 | Total step num: 15800 |\n",
      "| Reward: -120 | Episode: 80 | Total step num: 16000 |\n",
      "| Reward: -122 | Episode: 81 | Total step num: 16200 |\n",
      "| Reward: -117 | Episode: 82 | Total step num: 16400 |\n",
      "| Reward: 0 | Episode: 83 | Total step num: 16600 |\n",
      "| Reward: -1 | Episode: 84 | Total step num: 16800 |\n",
      "| Reward: -239 | Episode: 85 | Total step num: 17000 |\n",
      "| Reward: -243 | Episode: 86 | Total step num: 17200 |\n",
      "| Reward: -119 | Episode: 87 | Total step num: 17400 |\n",
      "| Reward: -227 | Episode: 88 | Total step num: 17600 |\n",
      "| Reward: 0 | Episode: 89 | Total step num: 17800 |\n",
      "| Reward: -119 | Episode: 90 | Total step num: 18000 |\n",
      "| Reward: -115 | Episode: 91 | Total step num: 18200 |\n",
      "| Reward: -345 | Episode: 92 | Total step num: 18400 |\n",
      "| Reward: -124 | Episode: 93 | Total step num: 18600 |\n",
      "| Reward: -296 | Episode: 94 | Total step num: 18800 |\n",
      "| Reward: -125 | Episode: 95 | Total step num: 19000 |\n",
      "| Reward: -355 | Episode: 96 | Total step num: 19200 |\n",
      "| Reward: -1 | Episode: 97 | Total step num: 19400 |\n",
      "| Reward: -122 | Episode: 98 | Total step num: 19600 |\n",
      "| Reward: -234 | Episode: 99 | Total step num: 19800 |\n",
      "total_step_cnt 20000\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -241.556\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -345.630\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -119.567\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -118.812\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -233.951\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -116.998\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -117.511\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -238.220\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -119.109\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -229.007\n",
      "---------------------------------------\n",
      "return_test[4] -188\n",
      "| Reward: -125 | Episode: 100 | Total step num: 20000 |\n",
      "| Reward: -320 | Episode: 101 | Total step num: 20200 |\n",
      "| Reward: -125 | Episode: 102 | Total step num: 20400 |\n",
      "| Reward: -118 | Episode: 103 | Total step num: 20600 |\n",
      "| Reward: -241 | Episode: 104 | Total step num: 20800 |\n",
      "| Reward: -123 | Episode: 105 | Total step num: 21000 |\n",
      "| Reward: -120 | Episode: 106 | Total step num: 21200 |\n",
      "| Reward: -1 | Episode: 107 | Total step num: 21400 |\n",
      "| Reward: -228 | Episode: 108 | Total step num: 21600 |\n",
      "| Reward: -349 | Episode: 109 | Total step num: 21800 |\n",
      "| Reward: -234 | Episode: 110 | Total step num: 22000 |\n",
      "| Reward: -116 | Episode: 111 | Total step num: 22200 |\n",
      "| Reward: -116 | Episode: 112 | Total step num: 22400 |\n",
      "| Reward: -231 | Episode: 113 | Total step num: 22600 |\n",
      "| Reward: -236 | Episode: 114 | Total step num: 22800 |\n",
      "| Reward: -234 | Episode: 115 | Total step num: 23000 |\n",
      "| Reward: -120 | Episode: 116 | Total step num: 23200 |\n",
      "| Reward: 0 | Episode: 117 | Total step num: 23400 |\n",
      "| Reward: -125 | Episode: 118 | Total step num: 23600 |\n",
      "| Reward: -122 | Episode: 119 | Total step num: 23800 |\n",
      "| Reward: -2 | Episode: 120 | Total step num: 24000 |\n",
      "| Reward: -1 | Episode: 121 | Total step num: 24200 |\n",
      "| Reward: -126 | Episode: 122 | Total step num: 24400 |\n",
      "| Reward: -233 | Episode: 123 | Total step num: 24600 |\n",
      "| Reward: -116 | Episode: 124 | Total step num: 24800 |\n",
      "total_step_cnt 25000\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -340.321\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -121.364\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -119.325\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -233.010\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -118.688\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -117.892\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -237.099\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -119.096\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -227.956\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -120.711\n",
      "---------------------------------------\n",
      "return_test[5] -175\n",
      "| Reward: -122 | Episode: 125 | Total step num: 25000 |\n",
      "The result of the trial no.0 was saved.\n",
      "Trial Number: 1\n",
      "action_space.shape (1,)\n",
      "observation_space.shape (3,)\n",
      "total_step_cnt 1\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1233.113\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1458.099\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1491.027\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1256.627\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1810.927\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1403.697\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1541.172\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1547.578\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1342.275\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1522.307\n",
      "---------------------------------------\n",
      "return_test[0] -1460\n",
      "| Reward: -1306 | Episode: 1 | Total step num: 200 |\n",
      "| Reward: -888 | Episode: 2 | Total step num: 400 |\n",
      "| Reward: -769 | Episode: 3 | Total step num: 600 |\n",
      "| Reward: -1534 | Episode: 4 | Total step num: 800 |\n",
      "| Reward: -865 | Episode: 5 | Total step num: 1000 |\n",
      "| Reward: -1599 | Episode: 6 | Total step num: 1200 |\n",
      "| Reward: -1311 | Episode: 7 | Total step num: 1400 |\n",
      "| Reward: -1298 | Episode: 8 | Total step num: 1600 |\n",
      "| Reward: -1557 | Episode: 9 | Total step num: 1800 |\n",
      "| Reward: -1614 | Episode: 10 | Total step num: 2000 |\n",
      "| Reward: -1302 | Episode: 11 | Total step num: 2200 |\n",
      "| Reward: -872 | Episode: 12 | Total step num: 2400 |\n",
      "| Reward: -1054 | Episode: 13 | Total step num: 2600 |\n",
      "| Reward: -1368 | Episode: 14 | Total step num: 2800 |\n",
      "| Reward: -1576 | Episode: 15 | Total step num: 3000 |\n",
      "| Reward: -971 | Episode: 16 | Total step num: 3200 |\n",
      "| Reward: -1204 | Episode: 17 | Total step num: 3400 |\n",
      "| Reward: -1727 | Episode: 18 | Total step num: 3600 |\n",
      "| Reward: -895 | Episode: 19 | Total step num: 3800 |\n",
      "| Reward: -1073 | Episode: 20 | Total step num: 4000 |\n",
      "| Reward: -1812 | Episode: 21 | Total step num: 4200 |\n",
      "| Reward: -1012 | Episode: 22 | Total step num: 4400 |\n",
      "| Reward: -1723 | Episode: 23 | Total step num: 4600 |\n",
      "| Reward: -1047 | Episode: 24 | Total step num: 4800 |\n",
      "total_step_cnt 5000\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1458.099\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1491.027\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1256.627\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1810.927\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1403.697\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1541.172\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1547.578\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1342.275\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1522.307\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1538.181\n",
      "---------------------------------------\n",
      "return_test[1] -1491\n",
      "| Reward: -1630 | Episode: 25 | Total step num: 5000 |\n",
      "| Reward: -790 | Episode: 26 | Total step num: 5200 |\n",
      "| Reward: -1058 | Episode: 27 | Total step num: 5400 |\n",
      "| Reward: -1418 | Episode: 28 | Total step num: 5600 |\n",
      "| Reward: -1027 | Episode: 29 | Total step num: 5800 |\n",
      "| Reward: -1560 | Episode: 30 | Total step num: 6000 |\n",
      "| Reward: -966 | Episode: 31 | Total step num: 6200 |\n",
      "| Reward: -1184 | Episode: 32 | Total step num: 6400 |\n",
      "| Reward: -1664 | Episode: 33 | Total step num: 6600 |\n",
      "| Reward: -1492 | Episode: 34 | Total step num: 6800 |\n",
      "| Reward: -890 | Episode: 35 | Total step num: 7000 |\n",
      "| Reward: -1600 | Episode: 36 | Total step num: 7200 |\n",
      "| Reward: -1344 | Episode: 37 | Total step num: 7400 |\n",
      "| Reward: -1064 | Episode: 38 | Total step num: 7600 |\n",
      "| Reward: -1073 | Episode: 39 | Total step num: 7800 |\n",
      "| Reward: -897 | Episode: 40 | Total step num: 8000 |\n",
      "| Reward: -1479 | Episode: 41 | Total step num: 8200 |\n",
      "| Reward: -1515 | Episode: 42 | Total step num: 8400 |\n",
      "| Reward: -1371 | Episode: 43 | Total step num: 8600 |\n",
      "| Reward: -636 | Episode: 44 | Total step num: 8800 |\n",
      "| Reward: -1050 | Episode: 45 | Total step num: 9000 |\n",
      "| Reward: -1788 | Episode: 46 | Total step num: 9200 |\n",
      "| Reward: -1295 | Episode: 47 | Total step num: 9400 |\n",
      "| Reward: -959 | Episode: 48 | Total step num: 9600 |\n",
      "| Reward: -1070 | Episode: 49 | Total step num: 9800 |\n",
      "total_step_cnt 10000\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1491.027\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1256.627\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1810.927\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1403.697\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1541.172\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1547.578\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1342.275\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1522.307\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1538.181\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1553.272\n",
      "---------------------------------------\n",
      "return_test[2] -1500\n",
      "| Reward: -1081 | Episode: 50 | Total step num: 10000 |\n",
      "| Reward: -1240 | Episode: 51 | Total step num: 10200 |\n",
      "| Reward: -1589 | Episode: 52 | Total step num: 10400 |\n",
      "| Reward: -1267 | Episode: 53 | Total step num: 10600 |\n",
      "| Reward: -1343 | Episode: 54 | Total step num: 10800 |\n",
      "| Reward: -1546 | Episode: 55 | Total step num: 11000 |\n",
      "| Reward: -1317 | Episode: 56 | Total step num: 11200 |\n",
      "| Reward: -1456 | Episode: 57 | Total step num: 11400 |\n",
      "| Reward: -1227 | Episode: 58 | Total step num: 11600 |\n",
      "| Reward: -1278 | Episode: 59 | Total step num: 11800 |\n",
      "| Reward: -1309 | Episode: 60 | Total step num: 12000 |\n",
      "| Reward: -1035 | Episode: 61 | Total step num: 12200 |\n",
      "| Reward: -973 | Episode: 62 | Total step num: 12400 |\n",
      "| Reward: -386 | Episode: 63 | Total step num: 12600 |\n",
      "| Reward: -514 | Episode: 64 | Total step num: 12800 |\n",
      "| Reward: -130 | Episode: 65 | Total step num: 13000 |\n",
      "| Reward: -420 | Episode: 66 | Total step num: 13200 |\n",
      "| Reward: -471 | Episode: 67 | Total step num: 13400 |\n",
      "| Reward: 0 | Episode: 68 | Total step num: 13600 |\n",
      "| Reward: -127 | Episode: 69 | Total step num: 13800 |\n",
      "| Reward: -129 | Episode: 70 | Total step num: 14000 |\n",
      "| Reward: -346 | Episode: 71 | Total step num: 14200 |\n",
      "| Reward: -235 | Episode: 72 | Total step num: 14400 |\n",
      "| Reward: -119 | Episode: 73 | Total step num: 14600 |\n",
      "| Reward: -251 | Episode: 74 | Total step num: 14800 |\n",
      "total_step_cnt 15000\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -244.269\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -335.631\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -120.298\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -121.833\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -355.180\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -117.627\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -119.639\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -360.281\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -120.218\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -347.017\n",
      "---------------------------------------\n",
      "return_test[3] -224\n",
      "| Reward: -244 | Episode: 75 | Total step num: 15000 |\n",
      "| Reward: -123 | Episode: 76 | Total step num: 15200 |\n",
      "| Reward: -118 | Episode: 77 | Total step num: 15400 |\n",
      "| Reward: -114 | Episode: 78 | Total step num: 15600 |\n",
      "| Reward: -120 | Episode: 79 | Total step num: 15800 |\n",
      "| Reward: -122 | Episode: 80 | Total step num: 16000 |\n",
      "| Reward: -117 | Episode: 81 | Total step num: 16200 |\n",
      "| Reward: 0 | Episode: 82 | Total step num: 16400 |\n",
      "| Reward: -1 | Episode: 83 | Total step num: 16600 |\n",
      "| Reward: -241 | Episode: 84 | Total step num: 16800 |\n",
      "| Reward: -238 | Episode: 85 | Total step num: 17000 |\n",
      "| Reward: -118 | Episode: 86 | Total step num: 17200 |\n",
      "| Reward: -229 | Episode: 87 | Total step num: 17400 |\n",
      "| Reward: 0 | Episode: 88 | Total step num: 17600 |\n",
      "| Reward: -119 | Episode: 89 | Total step num: 17800 |\n",
      "| Reward: -115 | Episode: 90 | Total step num: 18000 |\n",
      "| Reward: -356 | Episode: 91 | Total step num: 18200 |\n",
      "| Reward: -124 | Episode: 92 | Total step num: 18400 |\n",
      "| Reward: -287 | Episode: 93 | Total step num: 18600 |\n",
      "| Reward: -124 | Episode: 94 | Total step num: 18800 |\n",
      "| Reward: -349 | Episode: 95 | Total step num: 19000 |\n",
      "| Reward: -1 | Episode: 96 | Total step num: 19200 |\n",
      "| Reward: -121 | Episode: 97 | Total step num: 19400 |\n",
      "| Reward: -231 | Episode: 98 | Total step num: 19600 |\n",
      "| Reward: -124 | Episode: 99 | Total step num: 19800 |\n",
      "total_step_cnt 20000\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -349.599\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -119.308\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -118.695\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -235.078\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -116.214\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -117.413\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -238.536\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -118.970\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -230.133\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -118.814\n",
      "---------------------------------------\n",
      "return_test[4] -176\n",
      "| Reward: -320 | Episode: 100 | Total step num: 20000 |\n",
      "| Reward: -125 | Episode: 101 | Total step num: 20200 |\n",
      "| Reward: -118 | Episode: 102 | Total step num: 20400 |\n",
      "| Reward: -242 | Episode: 103 | Total step num: 20600 |\n",
      "| Reward: -122 | Episode: 104 | Total step num: 20800 |\n",
      "| Reward: -121 | Episode: 105 | Total step num: 21000 |\n",
      "| Reward: -1 | Episode: 106 | Total step num: 21200 |\n",
      "| Reward: -227 | Episode: 107 | Total step num: 21400 |\n",
      "| Reward: -345 | Episode: 108 | Total step num: 21600 |\n",
      "| Reward: -234 | Episode: 109 | Total step num: 21800 |\n",
      "| Reward: -117 | Episode: 110 | Total step num: 22000 |\n",
      "| Reward: -116 | Episode: 111 | Total step num: 22200 |\n",
      "| Reward: -234 | Episode: 112 | Total step num: 22400 |\n",
      "| Reward: -236 | Episode: 113 | Total step num: 22600 |\n",
      "| Reward: -237 | Episode: 114 | Total step num: 22800 |\n",
      "| Reward: -120 | Episode: 115 | Total step num: 23000 |\n",
      "| Reward: 0 | Episode: 116 | Total step num: 23200 |\n",
      "| Reward: -124 | Episode: 117 | Total step num: 23400 |\n",
      "| Reward: -122 | Episode: 118 | Total step num: 23600 |\n",
      "| Reward: -2 | Episode: 119 | Total step num: 23800 |\n",
      "| Reward: 0 | Episode: 120 | Total step num: 24000 |\n",
      "| Reward: -125 | Episode: 121 | Total step num: 24200 |\n",
      "| Reward: -234 | Episode: 122 | Total step num: 24400 |\n",
      "| Reward: -117 | Episode: 123 | Total step num: 24600 |\n",
      "| Reward: -122 | Episode: 124 | Total step num: 24800 |\n",
      "total_step_cnt 25000\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -119.189\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -120.351\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -234.437\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -116.576\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -118.560\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -237.905\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -119.413\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -229.491\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -120.137\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -335.352\n",
      "---------------------------------------\n",
      "return_test[5] -175\n",
      "| Reward: -118 | Episode: 125 | Total step num: 25000 |\n",
      "The result of the trial no.1 was saved.\n",
      "Trial Number: 2\n",
      "action_space.shape (1,)\n",
      "observation_space.shape (3,)\n",
      "total_step_cnt 1\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1347.928\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1159.496\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1196.689\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1779.724\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1310.271\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1185.177\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1589.212\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1235.048\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1200.899\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1586.512\n",
      "---------------------------------------\n",
      "return_test[0] -1359\n",
      "| Reward: -985 | Episode: 1 | Total step num: 200 |\n",
      "| Reward: -796 | Episode: 2 | Total step num: 400 |\n",
      "| Reward: -1756 | Episode: 3 | Total step num: 600 |\n",
      "| Reward: -754 | Episode: 4 | Total step num: 800 |\n",
      "| Reward: -1678 | Episode: 5 | Total step num: 1000 |\n",
      "| Reward: -1353 | Episode: 6 | Total step num: 1200 |\n",
      "| Reward: -1241 | Episode: 7 | Total step num: 1400 |\n",
      "| Reward: -1480 | Episode: 8 | Total step num: 1600 |\n",
      "| Reward: -1618 | Episode: 9 | Total step num: 1800 |\n",
      "| Reward: -1367 | Episode: 10 | Total step num: 2000 |\n",
      "| Reward: -1171 | Episode: 11 | Total step num: 2200 |\n",
      "| Reward: -1237 | Episode: 12 | Total step num: 2400 |\n",
      "| Reward: -1317 | Episode: 13 | Total step num: 2600 |\n",
      "| Reward: -1700 | Episode: 14 | Total step num: 2800 |\n",
      "| Reward: -951 | Episode: 15 | Total step num: 3000 |\n",
      "| Reward: -1245 | Episode: 16 | Total step num: 3200 |\n",
      "| Reward: -1692 | Episode: 17 | Total step num: 3400 |\n",
      "| Reward: -957 | Episode: 18 | Total step num: 3600 |\n",
      "| Reward: -1253 | Episode: 19 | Total step num: 3800 |\n",
      "| Reward: -1778 | Episode: 20 | Total step num: 4000 |\n",
      "| Reward: -891 | Episode: 21 | Total step num: 4200 |\n",
      "| Reward: -1655 | Episode: 22 | Total step num: 4400 |\n",
      "| Reward: -1127 | Episode: 23 | Total step num: 4600 |\n",
      "| Reward: -1750 | Episode: 24 | Total step num: 4800 |\n",
      "total_step_cnt 5000\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1159.496\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1196.689\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1779.724\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1310.271\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1185.177\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1589.212\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1235.048\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1200.899\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1586.512\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1192.425\n",
      "---------------------------------------\n",
      "return_test[1] -1343\n",
      "| Reward: -1015 | Episode: 25 | Total step num: 5000 |\n",
      "| Reward: -1169 | Episode: 26 | Total step num: 5200 |\n",
      "| Reward: -1778 | Episode: 27 | Total step num: 5400 |\n",
      "| Reward: -1079 | Episode: 28 | Total step num: 5600 |\n",
      "| Reward: -1595 | Episode: 29 | Total step num: 5800 |\n",
      "| Reward: -754 | Episode: 30 | Total step num: 6000 |\n",
      "| Reward: -1183 | Episode: 31 | Total step num: 6200 |\n",
      "| Reward: -1763 | Episode: 32 | Total step num: 6400 |\n",
      "| Reward: -1573 | Episode: 33 | Total step num: 6600 |\n",
      "| Reward: -1074 | Episode: 34 | Total step num: 6800 |\n",
      "| Reward: -1364 | Episode: 35 | Total step num: 7000 |\n",
      "| Reward: -1362 | Episode: 36 | Total step num: 7200 |\n",
      "| Reward: -1016 | Episode: 37 | Total step num: 7400 |\n",
      "| Reward: -861 | Episode: 38 | Total step num: 7600 |\n",
      "| Reward: -812 | Episode: 39 | Total step num: 7800 |\n",
      "| Reward: -1489 | Episode: 40 | Total step num: 8000 |\n",
      "| Reward: -1690 | Episode: 41 | Total step num: 8200 |\n",
      "| Reward: -1347 | Episode: 42 | Total step num: 8400 |\n",
      "| Reward: -854 | Episode: 43 | Total step num: 8600 |\n",
      "| Reward: -1029 | Episode: 44 | Total step num: 8800 |\n",
      "| Reward: -1682 | Episode: 45 | Total step num: 9000 |\n",
      "| Reward: -1364 | Episode: 46 | Total step num: 9200 |\n",
      "| Reward: -1239 | Episode: 47 | Total step num: 9400 |\n",
      "| Reward: -770 | Episode: 48 | Total step num: 9600 |\n",
      "| Reward: -1233 | Episode: 49 | Total step num: 9800 |\n",
      "total_step_cnt 10000\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1196.689\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1779.724\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1310.271\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1185.177\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1589.212\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1235.048\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1200.899\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1586.512\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1192.425\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1590.866\n",
      "---------------------------------------\n",
      "return_test[2] -1386\n",
      "| Reward: -1166 | Episode: 50 | Total step num: 10000 |\n",
      "| Reward: -1333 | Episode: 51 | Total step num: 10200 |\n",
      "| Reward: -1083 | Episode: 52 | Total step num: 10400 |\n",
      "| Reward: -1279 | Episode: 53 | Total step num: 10600 |\n",
      "| Reward: -1567 | Episode: 54 | Total step num: 10800 |\n",
      "| Reward: -1317 | Episode: 55 | Total step num: 11000 |\n",
      "| Reward: -1391 | Episode: 56 | Total step num: 11200 |\n",
      "| Reward: -1372 | Episode: 57 | Total step num: 11400 |\n",
      "| Reward: -1580 | Episode: 58 | Total step num: 11600 |\n",
      "| Reward: -1388 | Episode: 59 | Total step num: 11800 |\n",
      "| Reward: -1044 | Episode: 60 | Total step num: 12000 |\n",
      "| Reward: -1006 | Episode: 61 | Total step num: 12200 |\n",
      "| Reward: -129 | Episode: 62 | Total step num: 12400 |\n",
      "| Reward: -639 | Episode: 63 | Total step num: 12600 |\n",
      "| Reward: -385 | Episode: 64 | Total step num: 12800 |\n",
      "| Reward: -420 | Episode: 65 | Total step num: 13000 |\n",
      "| Reward: -471 | Episode: 66 | Total step num: 13200 |\n",
      "| Reward: 0 | Episode: 67 | Total step num: 13400 |\n",
      "| Reward: -127 | Episode: 68 | Total step num: 13600 |\n",
      "| Reward: -127 | Episode: 69 | Total step num: 13800 |\n",
      "| Reward: -351 | Episode: 70 | Total step num: 14000 |\n",
      "| Reward: -236 | Episode: 71 | Total step num: 14200 |\n",
      "| Reward: -122 | Episode: 72 | Total step num: 14400 |\n",
      "| Reward: -251 | Episode: 73 | Total step num: 14600 |\n",
      "| Reward: -245 | Episode: 74 | Total step num: 14800 |\n",
      "total_step_cnt 15000\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -335.535\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -120.074\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -122.920\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -879.561\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -117.761\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -119.788\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -887.219\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -122.796\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -348.824\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -122.265\n",
      "---------------------------------------\n",
      "return_test[3] -317\n",
      "| Reward: -254 | Episode: 75 | Total step num: 15000 |\n",
      "| Reward: -119 | Episode: 76 | Total step num: 15200 |\n",
      "| Reward: -115 | Episode: 77 | Total step num: 15400 |\n",
      "| Reward: -1185 | Episode: 78 | Total step num: 15600 |\n",
      "| Reward: -122 | Episode: 79 | Total step num: 15800 |\n",
      "| Reward: -117 | Episode: 80 | Total step num: 16000 |\n",
      "| Reward: -1 | Episode: 81 | Total step num: 16200 |\n",
      "| Reward: -1 | Episode: 82 | Total step num: 16400 |\n",
      "| Reward: -238 | Episode: 83 | Total step num: 16600 |\n",
      "| Reward: -238 | Episode: 84 | Total step num: 16800 |\n",
      "| Reward: -119 | Episode: 85 | Total step num: 17000 |\n",
      "| Reward: -227 | Episode: 86 | Total step num: 17200 |\n",
      "| Reward: -1 | Episode: 87 | Total step num: 17400 |\n",
      "| Reward: -119 | Episode: 88 | Total step num: 17600 |\n",
      "| Reward: -115 | Episode: 89 | Total step num: 17800 |\n",
      "| Reward: -352 | Episode: 90 | Total step num: 18000 |\n",
      "| Reward: -125 | Episode: 91 | Total step num: 18200 |\n",
      "| Reward: -296 | Episode: 92 | Total step num: 18400 |\n",
      "| Reward: -126 | Episode: 93 | Total step num: 18600 |\n",
      "| Reward: -348 | Episode: 94 | Total step num: 18800 |\n",
      "| Reward: -2 | Episode: 95 | Total step num: 19000 |\n",
      "| Reward: -121 | Episode: 96 | Total step num: 19200 |\n",
      "| Reward: -235 | Episode: 97 | Total step num: 19400 |\n",
      "| Reward: -125 | Episode: 98 | Total step num: 19600 |\n",
      "| Reward: -324 | Episode: 99 | Total step num: 19800 |\n",
      "total_step_cnt 20000\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -119.143\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -119.933\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -235.896\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -116.345\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -118.071\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -239.411\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -118.848\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -230.868\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -118.613\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -334.050\n",
      "---------------------------------------\n",
      "return_test[4] -175\n",
      "| Reward: -125 | Episode: 100 | Total step num: 20000 |\n",
      "| Reward: -119 | Episode: 101 | Total step num: 20200 |\n",
      "| Reward: -242 | Episode: 102 | Total step num: 20400 |\n",
      "| Reward: -121 | Episode: 103 | Total step num: 20600 |\n",
      "| Reward: -121 | Episode: 104 | Total step num: 20800 |\n",
      "| Reward: -1 | Episode: 105 | Total step num: 21000 |\n",
      "| Reward: -227 | Episode: 106 | Total step num: 21200 |\n",
      "| Reward: -342 | Episode: 107 | Total step num: 21400 |\n",
      "| Reward: -234 | Episode: 108 | Total step num: 21600 |\n",
      "| Reward: -116 | Episode: 109 | Total step num: 21800 |\n",
      "| Reward: -116 | Episode: 110 | Total step num: 22000 |\n",
      "| Reward: -233 | Episode: 111 | Total step num: 22200 |\n",
      "| Reward: -236 | Episode: 112 | Total step num: 22400 |\n",
      "| Reward: -236 | Episode: 113 | Total step num: 22600 |\n",
      "| Reward: -120 | Episode: 114 | Total step num: 22800 |\n",
      "| Reward: -1 | Episode: 115 | Total step num: 23000 |\n",
      "| Reward: -125 | Episode: 116 | Total step num: 23200 |\n",
      "| Reward: -122 | Episode: 117 | Total step num: 23400 |\n",
      "| Reward: -2 | Episode: 118 | Total step num: 23600 |\n",
      "| Reward: -1 | Episode: 119 | Total step num: 23800 |\n",
      "| Reward: -126 | Episode: 120 | Total step num: 24000 |\n",
      "| Reward: -233 | Episode: 121 | Total step num: 24200 |\n",
      "| Reward: -116 | Episode: 122 | Total step num: 24400 |\n",
      "| Reward: -121 | Episode: 123 | Total step num: 24600 |\n",
      "| Reward: -118 | Episode: 124 | Total step num: 24800 |\n",
      "total_step_cnt 25000\n",
      "evaluating the deterministic policy...\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -119.417\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -233.059\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -116.270\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -118.077\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -237.358\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -119.268\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -227.551\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -118.569\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -336.588\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -0.748\n",
      "---------------------------------------\n",
      "return_test[5] -162\n",
      "| Reward: -247 | Episode: 125 | Total step num: 25000 |\n",
      "The result of the trial no.2 was saved.\n"
     ]
    }
   ],
   "source": [
    "def set_all_seeds(seed=1234):\n",
    "    \"\"\"set all random seed\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    import os\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "set_all_seeds(1234)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA not available, using CPU\")\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# run parameters\n",
    "parser.add_argument('--env', help='choose the gym env- tested on {Pendulum-v1}')\n",
    "parser.add_argument('--env-id', type=int, default=0, help='choose the gym env- tested on {Pendulum-v1}')\n",
    "parser.add_argument('--random-seed', help='random seed for repeatability', default=1)\n",
    "parser.add_argument('--max-episodes', help='max num of episodes to do while training', default=1001)\n",
    "parser.add_argument('--max-episode-len', help='max length of 1 episode', default=1000)\n",
    "parser.add_argument('--trial-num', help='number of trials', default=3)\n",
    "parser.add_argument('--total-step-num', help='total number of time steps', default=25000)\n",
    "parser.add_argument('--eval-step-freq', help='frequency of evaluating the policy', default=5000)\n",
    "parser.add_argument('--test-num', help='number of test episodes', default=10)\n",
    "\n",
    "parser.add_argument('--result-file', help='file name for storing results from multiple trials',\n",
    "                    default='./results/trials/td3/trials_td3_')\n",
    "parser.add_argument('--trial-idx', help='index of trials', default=0)\n",
    "parser.add_argument('--monitor-dir', help='directory for recording', default='results/video/td3')\n",
    "\n",
    "parser.add_argument(\"--start_timesteps\", default=1e4, type=int)\n",
    "parser.add_argument(\"--expl_noise\", default=0.1, type=float)\n",
    "parser.add_argument(\"--batch_size\", default=256, type=int)\n",
    "parser.add_argument(\"--update_freq\", default=1, type=int)\n",
    "\n",
    "parser.set_defaults(render_env=False)\n",
    "\n",
    "args_tmp, unknown = parser.parse_known_args()\n",
    "\n",
    "if args_tmp.env is None:\n",
    "    env_dict = {0: \"Pendulum-v1\"}\n",
    "    args_tmp.env = env_dict[args_tmp.env_id]\n",
    "args = vars(args_tmp)\n",
    "\n",
    "return_set = []\n",
    "for ite in range(int(args['trial_num'])):\n",
    "    print('Trial Number:', ite)\n",
    "\n",
    "    seed = 1234 + ite\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    index = int(ite) + int(args['trial_idx'])\n",
    "    env = gym.make(args['env'])\n",
    "    env.reset(seed=seed)\n",
    "    env.action_space.seed(seed)\n",
    "\n",
    "    print('action_space.shape', env.action_space.shape)\n",
    "    print('observation_space.shape', env.observation_space.shape)\n",
    "    action_bound = float(env.action_space.high[0])\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "\n",
    "    agent = SAC(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        max_action=action_bound,\n",
    "        discount=0.99,    \n",
    "        tau=0.005,    \n",
    "        alpha=0.2,\n",
    "        target_entropy=None,\n",
    "        automatic_entropy_tuning=True\n",
    "    )\n",
    "\n",
    "    step_R_i = train(env, agent, args, index, seed)\n",
    "    return_set.append(step_R_i)\n",
    "\n",
    "    result_path = \"./results/trials/td3\"\n",
    "    result_filename = args['result_file'] +  \\\n",
    "                      '_update_freq_' + str(int(args['update_freq'])) + '_' + args['env'] +  \\\n",
    "                      '_trial_idx_' + str(index) + '.txt'\n",
    "    try:\n",
    "        import pathlib\n",
    "        pathlib.Path(result_path).mkdir(parents=True, exist_ok=True)\n",
    "        np.savetxt(result_filename, np.asarray(step_R_i))\n",
    "        print('The result of the trial no.' + str(index) + ' was saved.')\n",
    "    except:\n",
    "        print(\"A result directory does not exist and cannot be created. The trial results are not saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "figure(figsize=(7, 6))\n",
    "\n",
    "t = np.arange(0, int(args['total_step_num']) + 1, int(args['eval_step_freq'])) * 0.001\n",
    "\n",
    "mean = np.mean(np.asarray(return_set), axis=0)\n",
    "std = np.std(np.asarray(return_set), axis=0)\n",
    "color = 'b'\n",
    "label = 'SAC'\n",
    "plt.plot(t, mean, color, label=label)\n",
    "plt.fill(np.concatenate([t, t[::-1]]), np.concatenate([mean - 1.9600 * std,\n",
    "                                      (mean + 1.9600 * std)[::-1]]), alpha=.1, fc=color, ec='None')\n",
    "\n",
    "plt.xlabel('Time steps [x 1e3]', fontsize=14)\n",
    "plt.ylabel('Return', fontsize=14)\n",
    "plt.legend(loc='lower right', fontsize=14)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "inv_opt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
